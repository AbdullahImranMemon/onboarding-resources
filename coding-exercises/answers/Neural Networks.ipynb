{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97939a22-8617-4428-a5cd-86e08c0c8bd2",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "From the previous lecture we have learnt the key concepts behind neural networks. To reinforce your understanding of these concepts we are once again looking at the **Rain in Australia (weatherAUS)** dataset as to create a model that predicts rainfall given meteorological features.\n",
    "\n",
    "## Imports\n",
    "Below are the associated imports for the project. Don't be afraid to add more if to improve the quality of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5b754b-ffaa-4625-939f-118128a9df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from typing import List, Dict, Tuple\n",
    "type LossFN = Union[Module]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3dae3d-fdc7-4b20-8b66-ce55a53ed0bc",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Lets have a look into what features and aspects are found within our dataset. A method we've used already is `head` however another useful one to keep note of is `info` which gives information on the amount of `NaN` values. Where a nan value is just any data that isn't available.\n",
    "\n",
    "For a more complex project it is advised to spend more time analysing the data as you may be able to extract important information useful for a model. For the sake of brevity we'll focus on filling and encoding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "934fe27d-309a-4a7c-8743-6640bee86112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
       "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
       "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
       "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
       "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
       "0           W           44.0          W  ...        71.0         22.0   \n",
       "1         WNW           44.0        NNW  ...        44.0         25.0   \n",
       "2         WSW           46.0          W  ...        38.0         30.0   \n",
       "3          NE           24.0         SE  ...        45.0         16.0   \n",
       "4           W           41.0        ENE  ...        82.0         33.0   \n",
       "\n",
       "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
       "0       1007.7       1007.1       8.0       NaN     16.9     21.8         No   \n",
       "1       1010.6       1007.8       NaN       NaN     17.2     24.3         No   \n",
       "2       1007.6       1008.7       NaN       2.0     21.0     23.2         No   \n",
       "3       1017.6       1012.8       NaN       NaN     18.1     26.5         No   \n",
       "4       1010.8       1006.0       7.0       8.0     17.8     29.7         No   \n",
       "\n",
       "   RainTomorrow  \n",
       "0            No  \n",
       "1            No  \n",
       "2            No  \n",
       "3            No  \n",
       "4            No  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"./data/weatherAUS.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eee66fa-a94b-44e7-ab5d-b111d5ea366a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 145460 entries, 0 to 145459\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   Date           145460 non-null  object \n",
      " 1   Location       145460 non-null  object \n",
      " 2   MinTemp        143975 non-null  float64\n",
      " 3   MaxTemp        144199 non-null  float64\n",
      " 4   Rainfall       142199 non-null  float64\n",
      " 5   Evaporation    82670 non-null   float64\n",
      " 6   Sunshine       75625 non-null   float64\n",
      " 7   WindGustDir    135134 non-null  object \n",
      " 8   WindGustSpeed  135197 non-null  float64\n",
      " 9   WindDir9am     134894 non-null  object \n",
      " 10  WindDir3pm     141232 non-null  object \n",
      " 11  WindSpeed9am   143693 non-null  float64\n",
      " 12  WindSpeed3pm   142398 non-null  float64\n",
      " 13  Humidity9am    142806 non-null  float64\n",
      " 14  Humidity3pm    140953 non-null  float64\n",
      " 15  Pressure9am    130395 non-null  float64\n",
      " 16  Pressure3pm    130432 non-null  float64\n",
      " 17  Cloud9am       89572 non-null   float64\n",
      " 18  Cloud3pm       86102 non-null   float64\n",
      " 19  Temp9am        143693 non-null  float64\n",
      " 20  Temp3pm        141851 non-null  float64\n",
      " 21  RainToday      142199 non-null  object \n",
      " 22  RainTomorrow   142193 non-null  object \n",
      "dtypes: float64(16), object(7)\n",
      "memory usage: 25.5+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55322746-6035-45a8-8760-b0c4fd7bdfdd",
   "metadata": {},
   "source": [
    "## **(Question 1)** Data-Preprocessing\n",
    "### **(Part A)** Feature Extraction\n",
    "Before we go about wrangling our data we first may want to choose features we believe are most important as to decrease the complexity of the training data. This will involves removing specific columns. Remember there are tradeoffs between both, with less features our model will train faster but at the cost of possibly missing important information. So when choosing be careful not to remove anything too vital. \n",
    "\n",
    "**Optionally remove columns to simplify the dataset**, make sure not to go overboard with this.\n",
    "\n",
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "    The function <code>dataset.drop([...], axis=1, inplace=True)</code> may be useful.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f5ca18-e40b-4a86-8687-01a8a0cb2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns if you want.\n",
    "dataset.drop([\"Date\"], axis=1, inplace=True) # Lazy Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288b382-d84d-4b58-b7c9-bbd5bdbf1d70",
   "metadata": {},
   "source": [
    "### **(Part B)** Not Available Data\n",
    "From running `info` and `head` above we may see many incomplete fields within our dataset. For example despite their being 145460 entries only 142199 have a rain today value. Our first order of business is correcting these mistakes. There are two main approaches to this:\n",
    "- Removing unused data.\n",
    "- Replacing unused data (possibly with a calculation such as mean or median).\n",
    "\n",
    "Be aware of the downsides of both strategies, when removing data you decrease its size which can mean you decrease the model's accuracy. However by filling the data you may be adding bias and simplifying its complexity. \n",
    "\n",
    "**Preprocess the data as to ensure there are no `NaN` values**.\n",
    "\n",
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "    Some functions which may be useful depending on the approach used are:\n",
    "    <ul>\n",
    "        <li><code>dataset[column].fillna(...)</code></li>\n",
    "        <li><code>dataset[column].dropna()</code></li>\n",
    "    </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e92b3aa-e4fe-43e4-b385-0d37f0747442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place modifications to the dataset here...\n",
    "numeric_columns = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm']\n",
    "for col in numeric_columns:\n",
    "    dataset[col].fillna(dataset[col].median())\n",
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71673953-3a1d-4ab9-bb4e-f37ad4f8795e",
   "metadata": {},
   "source": [
    "To verify that you've removed all `NaN` data ensure values are 0 by running the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f18dc16-2ec6-49c1-a540-390cc3c4e728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location         0\n",
       "MinTemp          0\n",
       "MaxTemp          0\n",
       "Rainfall         0\n",
       "Evaporation      0\n",
       "Sunshine         0\n",
       "WindGustDir      0\n",
       "WindGustSpeed    0\n",
       "WindDir9am       0\n",
       "WindDir3pm       0\n",
       "WindSpeed9am     0\n",
       "WindSpeed3pm     0\n",
       "Humidity9am      0\n",
       "Humidity3pm      0\n",
       "Pressure9am      0\n",
       "Pressure3pm      0\n",
       "Cloud9am         0\n",
       "Cloud3pm         0\n",
       "Temp9am          0\n",
       "Temp3pm          0\n",
       "RainToday        0\n",
       "RainTomorrow     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum() # Verify by counting na"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d32fb-d272-45c0-95e1-b2ac0c97eb2c",
   "metadata": {},
   "source": [
    "### **(Part $\\beta$)** Dates\n",
    "This part is optional depending on if you removed the dates. In order to make use of our dates better it is best we split them into individual arguments rather than keeping them as one value. This will be easier to train on and be more generalisable in the long term.\n",
    "\n",
    "**Split the dates into Days, Months, Years and remove Dates**. Given you haven't removed dates already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8417d992-d06e-4108-ab81-61001b17ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply modifications to the dataset here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a31cd75-09e6-46d8-9f93-5b3518843069",
   "metadata": {},
   "source": [
    "This will print nothing when you have no longer have a date field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee15d436-7071-42de-ae39-866a121f2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Date\" in dataset.columns:\n",
    "    print(\"Please remove date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128ff0b-a88d-498f-ad39-8f749373c105",
   "metadata": {},
   "source": [
    "### **(Part C)** Encoding Arguments\n",
    "Looking back at the `dataset.info()` we see that a lot of data we are reliant on has the datatype `object`rather than boolean/integer. This data is mostly string labels, such as compass directions. This will cause problems later on as to generate weights and biases we need to encode these as numeric values. This can be done through two main approaches:\n",
    "- Label encoding, where every category is encoded as an integer. This can be done automatically or done by using a relevant number to the data, such as the amount of appearences. \n",
    "- On hot encoding, where each category is represented as $n$ Boolean columns, where $n$ is the amount of categories. Below is an example with `RainToday`.\n",
    "\n",
    "| ID | RainToday |\n",
    "| ---- | ----- |\n",
    "| 0 | Yes |\n",
    "| 1 | No |\n",
    "\n",
    "| ID | Yes | No |\n",
    "| --- | ---- | ----- |\n",
    "| 0 | 1 | 0 |\n",
    "| 1 | 0 | 1 |\n",
    "\n",
    "**Encode object arguments as integer values**, keep in mind columns that you don't encode need to be dropped as objects aren't a valid in neural networks.\n",
    "\n",
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "    Some functions which may be useful depending on the approach used are:\n",
    "    <ul>\n",
    "        <li><code>pd.get_dummies</code></li>\n",
    "        <li><code>dataset[...].map({})</code></li>\n",
    "        <li><code>OneHotEncoder</code> from sklearn</li>\n",
    "        <li><code>LabelEncoder</code> from sklearn</li>\n",
    "    </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba0741ee-4e81-4f74-aa61-1b65423cfe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Dataset...\n",
    "for x in (\"RainToday\", \"RainTomorrow\"):\n",
    "    dataset[x] = dataset[x].map({\"Yes\": 1, \"No\": 0})\n",
    "try:\n",
    "    encoded_cols = pd.get_dummies(dataset[['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm']])\n",
    "    dataset = pd.concat([dataset, encoded_cols], axis=1)\n",
    "    dataset.drop(['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm'], axis=1, inplace=True)\n",
    "except: pass\n",
    "boolean_columns = dataset.select_dtypes(include=bool).columns\n",
    "dataset[boolean_columns] = dataset[boolean_columns].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab822e7-6db7-4764-8369-d77bd6fff43f",
   "metadata": {},
   "source": [
    "Given that you have gone about doing the above task nothing should be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c8cb22-fed4-49b9-844e-55d1c033ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_encoded_cols = dataset.select_dtypes(include=['object']).columns\n",
    "if len(non_encoded_cols) > 0:\n",
    "    print(f\"The following rows need to be encoded:\\n{non_encoded_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c8616b-9091-45e2-b551-74200cd3fb71",
   "metadata": {},
   "source": [
    "## Pytorch Datasets\n",
    "After ensuring our dataset has the appropriate preprocessing done we can now turn it into a format that can be used by Torch. This is done with the `dataset` abstract class provided by Torch. For now we won't go too indepth just know that the below code creates a class that is able to access our data and then puts it into a iterator that allows for it to be returned by calling the `next(data)` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff855b8e-c639-4686-ab20-5789ef69347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherAUS(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        \"\"\"Intialise the dataset with key information\"\"\"\n",
    "        # If you want to modularise your model nicely it is fairly common to put\n",
    "        # your initial preprocessing here. \n",
    "        labels = dataset[\"RainTomorrow\"] # Use Rain Tomorrow as output\n",
    "        d = data.drop([\"RainTomorrow\"], axis=1) # Remove it from dataset\n",
    "        self.data = torch.tensor(d.values, dtype=torch.float32) # Convert data into tensors\n",
    "        labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "        self.labels = labels.view(-1, 1) # Turn Tensor[64] to Tensor[64, 1]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Get length of the data\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"Return the respective data at the index\"\"\"\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Apply wrapper to the dataset\n",
    "data = WeatherAUS(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef70c1-5eba-4cc6-bdd6-3fd4ceb290a8",
   "metadata": {},
   "source": [
    "## **(Question 2)** Splitting Data\n",
    "Now that we have our dataset we need to split it into two sets, one for training and one for testing. This will enable us to use the training data to find weights and biases for our model, and the testing to see how accuracy and loss has changed between training sessions.\n",
    "\n",
    "**Split into a training dataset and a test dataset**, use variables `train_dataset` and `test_dataset` as output. Pytorch provides a useful function `random_split` which has been imported.\n",
    "\n",
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "    Remember that we can use <code>len(data)</code> to get the length of our dataset.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14e46b9a-eae7-49cd-87ad-34dc05e864e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8*len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_dataset, test_dataset = random_split(data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc5b30-2350-4820-b6c3-1e974b4afbce",
   "metadata": {},
   "source": [
    "After splitting the data we wrap it in Torch's DataLoader class to allow it to be iterated by a model. If all this was successful no error should be produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc5f988f-af20-4170-b42f-82e6212ccf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4205ed39-0dc3-49df-b727-ed3b886485b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttensor, tlabel = next(iter(test_dataloader))\n",
    "if ttensor.shape[0] != batch_size and tlabel.shape[0] != batch_size:\n",
    "    print(f\"Tensor incorrect with shape:\\n{ttensor.shape}\\n{tlabel.shape}\")\n",
    "    print(ttensor, \"\\n\", tlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa8083-7850-4610-8965-78a21a8b5248",
   "metadata": {},
   "source": [
    "Notice how the `batch_size` results in an array of 64 elements. Training in batches is a common as it is more memory efficient than loading the entire dataset and trains faster than only loading one element at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64030317-3cea-43b3-ab1b-68893903b7d3",
   "metadata": {},
   "source": [
    "## Device\n",
    "Before we can go about creating a model we need to make sure we know what device we are going to put the model on. The below snippet decides if the device being used is cuda, mps or the cpu. With modern GPU's being optimised for general purpose mathematical operations it can be efficiently used to train models. Keep in mind if you do use your gpu that the model size fits within the vram of the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63261f1c-2116-4621-b27a-d6bf4c133335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Find device being used\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c9518c-800f-4874-8fe6-917260aef5fe",
   "metadata": {},
   "source": [
    "## **(Question 3)** Model\n",
    "While you won't be able to analyse how your model performs yet it is still a good idea to learn the process of model creation. The below code provided is the start of a very basic model. Once you fix the issues indicated you can start to experiment with it.\n",
    "\n",
    "**Fill in the gaps and comeback to this section later to create a better model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6601cb01-81b5-4baf-95f2-f2b0fe535e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class WeatherNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(len(ttensor[0]), 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc5 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.sigmoid(self.fc5(x))\n",
    "        return x\n",
    "\n",
    "model = WeatherNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d922892-f2f4-49a2-adc3-3ffdbb5b12ed",
   "metadata": {},
   "source": [
    "## Loss Function & Optimiser\n",
    "Loss functions can be ran like `loss_fn(prediction, answer)`. Optimisers have the functions `optimiser.step()` to use computed gradient values to changes a models parameters and `optimiser.zero_grad()` to reset these gradients.\n",
    "\n",
    "These are both values that can be changed. With the optimiser determining the rate at which the model learns the parameters, and loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fc49945-f4f1-4c5e-a3b9-edc27407ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss() # Loss function\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.0001) # Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0171c4e9-6bb3-4ff6-ad71-73b307009b60",
   "metadata": {},
   "source": [
    "## **(Question 4)** Training\n",
    "In order for all the inner weights and biases inside of the neural network to be found we first need train the model. This process generally follows a process of:\n",
    "\n",
    "1. Set model to `model.train()`\n",
    "2. Iterate through batches within the data loader.\n",
    "3. Assign the iterated values to a device `X, y = X.to(device), y.to(device)`.\n",
    "4. Compute the prediction error (*forward pass*).\n",
    "5. Run backpropogation (*back pass*).\n",
    "\n",
    "**Finish the train function to follow along with the above instructions**.\n",
    "<details>\n",
    "  <summary>Forward Pass Hint</summary>\n",
    "    Remember how the ML models functioned, we started by computing the predicted model. For our problem we can simply run <code>model(...)</code>, followed by computing the loss.\n",
    "</details>\n",
    "<details>\n",
    "  <summary>Back Pass Hint</summary>\n",
    "    Backpropogation can be computed by using <code>loss.backward()</code>. These gradients can then be used to update parameters by running <code>optimiser.step()</code> before reseting gradient computations to zero <code>optimiser.zero_grad()</code>.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "023c6c2c-cc18-4e54-977b-3f6136d436f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader: DataLoader, model: WeatherNet, loss_fn: LossFN, optimiser: Optimizer):\n",
    "    n = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Print loss occasionally\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Batch {batch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe816b4-819a-4400-a8fe-c368488f4eb0",
   "metadata": {},
   "source": [
    "## **(Question 5)** Testing\n",
    "Once we have finished running training we can now test the model against our testing dataset. This will provide us with a look into its current loss and accuracy. This can be done similar to training. Just avoid the backpropogation step as our goal isn't to change the model as this stage.\n",
    "\n",
    "**Create a test function that computes the loss and accuracy**.\n",
    "\n",
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "    The loss and accuracy can be found by iterating through the data and adding the loss <code>loss_fn(pred, y)</code> and accuracy <code>pred.argmax(1)==y</code>.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "698fc62e-6c27-4a76-a670-0ce0ce259fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader: DataLoader, model: WeatherNet, loss_fn: LossFN):\n",
    "    \"\"\"\n",
    "    Test the model to see how it compares against the test portion of the\n",
    "    dataset\n",
    "    \"\"\"\n",
    "    n = len(dataloader.dataset)\n",
    "    n_batch = len(dataloader)\n",
    "    model.eval() # Set Torch to evaluation mode\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    with torch.no_grad(): # This step removes gradient descent modifications\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            acc += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    loss /= n_batch\n",
    "    acc /= n\n",
    "    print(f\"Test Error: \\nAccuracy: {(acc):>0.1f}%, Avg loss: {loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f609e58-6ddd-41ba-9ed1-42fd649e5a4e",
   "metadata": {},
   "source": [
    "## Results\n",
    "Given that the last two functions are correct you should be able to run the below loop. This will run the training and test phases creating a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56b57332-637c-4fb6-a8bd-24ed5a0cd8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.15621289610862732\n",
      "Batch 100, Loss: 0.2074325829744339\n",
      "Batch 200, Loss: 0.18956616520881653\n",
      "Batch 300, Loss: 0.21230560541152954\n",
      "Batch 400, Loss: 0.2714022696018219\n",
      "Batch 500, Loss: 0.19510801136493683\n",
      "Batch 600, Loss: 0.14476263523101807\n",
      "Batch 700, Loss: 0.2078814059495926\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.161335 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.1433093547821045\n",
      "Batch 100, Loss: 0.20733773708343506\n",
      "Batch 200, Loss: 0.21770885586738586\n",
      "Batch 300, Loss: 0.1936275064945221\n",
      "Batch 400, Loss: 0.24502038955688477\n",
      "Batch 500, Loss: 0.20431651175022125\n",
      "Batch 600, Loss: 0.15416623651981354\n",
      "Batch 700, Loss: 0.17253081500530243\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.161120 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.16956141591072083\n",
      "Batch 100, Loss: 0.21775877475738525\n",
      "Batch 200, Loss: 0.17557395994663239\n",
      "Batch 300, Loss: 0.21311374008655548\n",
      "Batch 400, Loss: 0.25120809674263\n",
      "Batch 500, Loss: 0.1822417825460434\n",
      "Batch 600, Loss: 0.1814379245042801\n",
      "Batch 700, Loss: 0.18217907845973969\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.160940 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.15157254040241241\n",
      "Batch 100, Loss: 0.2258262038230896\n",
      "Batch 200, Loss: 0.2235691249370575\n",
      "Batch 300, Loss: 0.21387216448783875\n",
      "Batch 400, Loss: 0.22821971774101257\n",
      "Batch 500, Loss: 0.18862393498420715\n",
      "Batch 600, Loss: 0.14873327314853668\n",
      "Batch 700, Loss: 0.20361174643039703\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.160104 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.1520223319530487\n",
      "Batch 100, Loss: 0.21095824241638184\n",
      "Batch 200, Loss: 0.19582073390483856\n",
      "Batch 300, Loss: 0.21560274064540863\n",
      "Batch 400, Loss: 0.27021461725234985\n",
      "Batch 500, Loss: 0.21153217554092407\n",
      "Batch 600, Loss: 0.1392865628004074\n",
      "Batch 700, Loss: 0.17014577984809875\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.159779 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.14556053280830383\n",
      "Batch 100, Loss: 0.2252490520477295\n",
      "Batch 200, Loss: 0.1852717399597168\n",
      "Batch 300, Loss: 0.20025178790092468\n",
      "Batch 400, Loss: 0.23687438666820526\n",
      "Batch 500, Loss: 0.17332738637924194\n",
      "Batch 600, Loss: 0.12616004049777985\n",
      "Batch 700, Loss: 0.18131576478481293\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.159416 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.1255149394273758\n",
      "Batch 100, Loss: 0.2228938341140747\n",
      "Batch 200, Loss: 0.19552868604660034\n",
      "Batch 300, Loss: 0.20579394698143005\n",
      "Batch 400, Loss: 0.27697381377220154\n",
      "Batch 500, Loss: 0.20110583305358887\n",
      "Batch 600, Loss: 0.1378447711467743\n",
      "Batch 700, Loss: 0.18615105748176575\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.159070 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.16098164021968842\n",
      "Batch 100, Loss: 0.23126471042633057\n",
      "Batch 200, Loss: 0.16769494116306305\n",
      "Batch 300, Loss: 0.19011394679546356\n",
      "Batch 400, Loss: 0.26857101917266846\n",
      "Batch 500, Loss: 0.17989027500152588\n",
      "Batch 600, Loss: 0.13180933892726898\n",
      "Batch 700, Loss: 0.16051191091537476\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.158764 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.13892416656017303\n",
      "Batch 100, Loss: 0.2158309668302536\n",
      "Batch 200, Loss: 0.18913742899894714\n",
      "Batch 300, Loss: 0.20324555039405823\n",
      "Batch 400, Loss: 0.19083485007286072\n",
      "Batch 500, Loss: 0.17529606819152832\n",
      "Batch 600, Loss: 0.1253078430891037\n",
      "Batch 700, Loss: 0.1990451067686081\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.158712 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.1475331038236618\n",
      "Batch 100, Loss: 0.23487263917922974\n",
      "Batch 200, Loss: 0.18531852960586548\n",
      "Batch 300, Loss: 0.19035814702510834\n",
      "Batch 400, Loss: 0.2419600784778595\n",
      "Batch 500, Loss: 0.19974909722805023\n",
      "Batch 600, Loss: 0.141495943069458\n",
      "Batch 700, Loss: 0.17437519133090973\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.157748 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.1772158443927765\n",
      "Batch 100, Loss: 0.2226330190896988\n",
      "Batch 200, Loss: 0.18068301677703857\n",
      "Batch 300, Loss: 0.17909473180770874\n",
      "Batch 400, Loss: 0.25486838817596436\n",
      "Batch 500, Loss: 0.16995681822299957\n",
      "Batch 600, Loss: 0.15844281017780304\n",
      "Batch 700, Loss: 0.18212610483169556\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.157632 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.1419830024242401\n",
      "Batch 100, Loss: 0.23605500161647797\n",
      "Batch 200, Loss: 0.1831013262271881\n",
      "Batch 300, Loss: 0.1746956706047058\n",
      "Batch 400, Loss: 0.24078981578350067\n",
      "Batch 500, Loss: 0.1780063509941101\n",
      "Batch 600, Loss: 0.1328929364681244\n",
      "Batch 700, Loss: 0.162753626704216\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.156739 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.13240864872932434\n",
      "Batch 100, Loss: 0.23179662227630615\n",
      "Batch 200, Loss: 0.20263636112213135\n",
      "Batch 300, Loss: 0.1998535692691803\n",
      "Batch 400, Loss: 0.24315251410007477\n",
      "Batch 500, Loss: 0.16704577207565308\n",
      "Batch 600, Loss: 0.1375933289527893\n",
      "Batch 700, Loss: 0.18348917365074158\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.156500 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.12936407327651978\n",
      "Batch 100, Loss: 0.2233838587999344\n",
      "Batch 200, Loss: 0.17467360198497772\n",
      "Batch 300, Loss: 0.18900629878044128\n",
      "Batch 400, Loss: 0.23022623360157013\n",
      "Batch 500, Loss: 0.1969766616821289\n",
      "Batch 600, Loss: 0.12813431024551392\n",
      "Batch 700, Loss: 0.19684062898159027\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.155800 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.12305831909179688\n",
      "Batch 100, Loss: 0.21247810125350952\n",
      "Batch 200, Loss: 0.17692440748214722\n",
      "Batch 300, Loss: 0.19223102927207947\n",
      "Batch 400, Loss: 0.2390809804201126\n",
      "Batch 500, Loss: 0.17809146642684937\n",
      "Batch 600, Loss: 0.14292006194591522\n",
      "Batch 700, Loss: 0.1473468840122223\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.155381 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.15522444248199463\n",
      "Batch 100, Loss: 0.20856305956840515\n",
      "Batch 200, Loss: 0.19100907444953918\n",
      "Batch 300, Loss: 0.1785275638103485\n",
      "Batch 400, Loss: 0.23179227113723755\n",
      "Batch 500, Loss: 0.1590835154056549\n",
      "Batch 600, Loss: 0.1368163675069809\n",
      "Batch 700, Loss: 0.1959352046251297\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.154761 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.13603490591049194\n",
      "Batch 100, Loss: 0.22687874734401703\n",
      "Batch 200, Loss: 0.16431431472301483\n",
      "Batch 300, Loss: 0.1996440589427948\n",
      "Batch 400, Loss: 0.22072084248065948\n",
      "Batch 500, Loss: 0.16471433639526367\n",
      "Batch 600, Loss: 0.15085533261299133\n",
      "Batch 700, Loss: 0.18481764197349548\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.154394 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.1443425714969635\n",
      "Batch 100, Loss: 0.1768663376569748\n",
      "Batch 200, Loss: 0.21725547313690186\n",
      "Batch 300, Loss: 0.18204955756664276\n",
      "Batch 400, Loss: 0.23316451907157898\n",
      "Batch 500, Loss: 0.16091388463974\n",
      "Batch 600, Loss: 0.13564607501029968\n",
      "Batch 700, Loss: 0.17628717422485352\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.153780 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.1391768604516983\n",
      "Batch 100, Loss: 0.18387602269649506\n",
      "Batch 200, Loss: 0.2084549218416214\n",
      "Batch 300, Loss: 0.18665626645088196\n",
      "Batch 400, Loss: 0.23736613988876343\n",
      "Batch 500, Loss: 0.1781800538301468\n",
      "Batch 600, Loss: 0.14895829558372498\n",
      "Batch 700, Loss: 0.18073126673698425\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.153333 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.13509149849414825\n",
      "Batch 100, Loss: 0.20071811974048615\n",
      "Batch 200, Loss: 0.15737685561180115\n",
      "Batch 300, Loss: 0.2064843475818634\n",
      "Batch 400, Loss: 0.23915047943592072\n",
      "Batch 500, Loss: 0.15191340446472168\n",
      "Batch 600, Loss: 0.15638867020606995\n",
      "Batch 700, Loss: 0.18677130341529846\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.152565 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.14695972204208374\n",
      "Batch 100, Loss: 0.21310284733772278\n",
      "Batch 200, Loss: 0.20350147783756256\n",
      "Batch 300, Loss: 0.17716431617736816\n",
      "Batch 400, Loss: 0.23504669964313507\n",
      "Batch 500, Loss: 0.15771155059337616\n",
      "Batch 600, Loss: 0.15928135812282562\n",
      "Batch 700, Loss: 0.17065536975860596\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.152085 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.1433984786272049\n",
      "Batch 100, Loss: 0.2274780571460724\n",
      "Batch 200, Loss: 0.19063374400138855\n",
      "Batch 300, Loss: 0.19613860547542572\n",
      "Batch 400, Loss: 0.2491326630115509\n",
      "Batch 500, Loss: 0.18914517760276794\n",
      "Batch 600, Loss: 0.1401841640472412\n",
      "Batch 700, Loss: 0.16837362945079803\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.151320 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.1602572649717331\n",
      "Batch 100, Loss: 0.2139289379119873\n",
      "Batch 200, Loss: 0.1991613209247589\n",
      "Batch 300, Loss: 0.18227149546146393\n",
      "Batch 400, Loss: 0.2154192477464676\n",
      "Batch 500, Loss: 0.16982290148735046\n",
      "Batch 600, Loss: 0.13834279775619507\n",
      "Batch 700, Loss: 0.15611372888088226\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.151028 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.13486556708812714\n",
      "Batch 100, Loss: 0.19715704023838043\n",
      "Batch 200, Loss: 0.18875883519649506\n",
      "Batch 300, Loss: 0.18557599186897278\n",
      "Batch 400, Loss: 0.21394935250282288\n",
      "Batch 500, Loss: 0.16283339262008667\n",
      "Batch 600, Loss: 0.1406223326921463\n",
      "Batch 700, Loss: 0.16526435315608978\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.150156 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.148917093873024\n",
      "Batch 100, Loss: 0.1964220106601715\n",
      "Batch 200, Loss: 0.18846088647842407\n",
      "Batch 300, Loss: 0.18335279822349548\n",
      "Batch 400, Loss: 0.251016765832901\n",
      "Batch 500, Loss: 0.16916097700595856\n",
      "Batch 600, Loss: 0.13969630002975464\n",
      "Batch 700, Loss: 0.17213034629821777\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.149388 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.13514181971549988\n",
      "Batch 100, Loss: 0.2083851546049118\n",
      "Batch 200, Loss: 0.18933159112930298\n",
      "Batch 300, Loss: 0.1977970004081726\n",
      "Batch 400, Loss: 0.24407415091991425\n",
      "Batch 500, Loss: 0.1525202989578247\n",
      "Batch 600, Loss: 0.15823490917682648\n",
      "Batch 700, Loss: 0.1832195222377777\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.148893 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.12497327476739883\n",
      "Batch 100, Loss: 0.21627655625343323\n",
      "Batch 200, Loss: 0.17341798543930054\n",
      "Batch 300, Loss: 0.17676465213298798\n",
      "Batch 400, Loss: 0.2229115217924118\n",
      "Batch 500, Loss: 0.15518245100975037\n",
      "Batch 600, Loss: 0.14672774076461792\n",
      "Batch 700, Loss: 0.16601425409317017\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.148418 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.11843566596508026\n",
      "Batch 100, Loss: 0.21223227679729462\n",
      "Batch 200, Loss: 0.19449087977409363\n",
      "Batch 300, Loss: 0.176271453499794\n",
      "Batch 400, Loss: 0.25339192152023315\n",
      "Batch 500, Loss: 0.15742868185043335\n",
      "Batch 600, Loss: 0.14109697937965393\n",
      "Batch 700, Loss: 0.17194849252700806\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.147957 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.12849858403205872\n",
      "Batch 100, Loss: 0.20428183674812317\n",
      "Batch 200, Loss: 0.18409523367881775\n",
      "Batch 300, Loss: 0.1758626401424408\n",
      "Batch 400, Loss: 0.23829065263271332\n",
      "Batch 500, Loss: 0.18985314667224884\n",
      "Batch 600, Loss: 0.14372265338897705\n",
      "Batch 700, Loss: 0.18268254399299622\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.147118 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.14826008677482605\n",
      "Batch 100, Loss: 0.20562264323234558\n",
      "Batch 200, Loss: 0.18739645183086395\n",
      "Batch 300, Loss: 0.16868261992931366\n",
      "Batch 400, Loss: 0.2260744869709015\n",
      "Batch 500, Loss: 0.16505175828933716\n",
      "Batch 600, Loss: 0.16995209455490112\n",
      "Batch 700, Loss: 0.1569184511899948\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.146228 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.13784757256507874\n",
      "Batch 100, Loss: 0.18670493364334106\n",
      "Batch 200, Loss: 0.19238267838954926\n",
      "Batch 300, Loss: 0.17256179451942444\n",
      "Batch 400, Loss: 0.23532846570014954\n",
      "Batch 500, Loss: 0.14944790303707123\n",
      "Batch 600, Loss: 0.13547265529632568\n",
      "Batch 700, Loss: 0.16210785508155823\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.145690 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.1386406272649765\n",
      "Batch 100, Loss: 0.1783851981163025\n",
      "Batch 200, Loss: 0.1758463829755783\n",
      "Batch 300, Loss: 0.17145545780658722\n",
      "Batch 400, Loss: 0.21723537147045135\n",
      "Batch 500, Loss: 0.1562822461128235\n",
      "Batch 600, Loss: 0.1374172568321228\n",
      "Batch 700, Loss: 0.16547274589538574\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.144940 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.15645481646060944\n",
      "Batch 100, Loss: 0.21846678853034973\n",
      "Batch 200, Loss: 0.19354687631130219\n",
      "Batch 300, Loss: 0.18908098340034485\n",
      "Batch 400, Loss: 0.22281816601753235\n",
      "Batch 500, Loss: 0.1556728333234787\n",
      "Batch 600, Loss: 0.1393083781003952\n",
      "Batch 700, Loss: 0.17278490960597992\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.144773 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.15389131009578705\n",
      "Batch 100, Loss: 0.2120242714881897\n",
      "Batch 200, Loss: 0.1930505782365799\n",
      "Batch 300, Loss: 0.19021858274936676\n",
      "Batch 400, Loss: 0.2374911904335022\n",
      "Batch 500, Loss: 0.14094868302345276\n",
      "Batch 600, Loss: 0.1346403807401657\n",
      "Batch 700, Loss: 0.17203159630298615\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.143940 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.13197651505470276\n",
      "Batch 100, Loss: 0.2128499150276184\n",
      "Batch 200, Loss: 0.17054738104343414\n",
      "Batch 300, Loss: 0.14113876223564148\n",
      "Batch 400, Loss: 0.2503960132598877\n",
      "Batch 500, Loss: 0.16570042073726654\n",
      "Batch 600, Loss: 0.14614029228687286\n",
      "Batch 700, Loss: 0.17143365740776062\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.142887 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.15132704377174377\n",
      "Batch 100, Loss: 0.21717514097690582\n",
      "Batch 200, Loss: 0.1707499623298645\n",
      "Batch 300, Loss: 0.17844949662685394\n",
      "Batch 400, Loss: 0.2101953774690628\n",
      "Batch 500, Loss: 0.15408070385456085\n",
      "Batch 600, Loss: 0.13633599877357483\n",
      "Batch 700, Loss: 0.19907613098621368\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.142901 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.12749038636684418\n",
      "Batch 100, Loss: 0.2208022177219391\n",
      "Batch 200, Loss: 0.17386478185653687\n",
      "Batch 300, Loss: 0.18529564142227173\n",
      "Batch 400, Loss: 0.22619907557964325\n",
      "Batch 500, Loss: 0.14895759522914886\n",
      "Batch 600, Loss: 0.13856278359889984\n",
      "Batch 700, Loss: 0.18160733580589294\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.142161 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.13450132310390472\n",
      "Batch 100, Loss: 0.19158728420734406\n",
      "Batch 200, Loss: 0.1702624261379242\n",
      "Batch 300, Loss: 0.17706969380378723\n",
      "Batch 400, Loss: 0.23282930254936218\n",
      "Batch 500, Loss: 0.14210352301597595\n",
      "Batch 600, Loss: 0.13036498427391052\n",
      "Batch 700, Loss: 0.1748785674571991\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.140954 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.14823979139328003\n",
      "Batch 100, Loss: 0.22104984521865845\n",
      "Batch 200, Loss: 0.17741553485393524\n",
      "Batch 300, Loss: 0.17079848051071167\n",
      "Batch 400, Loss: 0.22804443538188934\n",
      "Batch 500, Loss: 0.147877499461174\n",
      "Batch 600, Loss: 0.14075584709644318\n",
      "Batch 700, Loss: 0.1612675040960312\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.140255 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.13792315125465393\n",
      "Batch 100, Loss: 0.19102826714515686\n",
      "Batch 200, Loss: 0.1807216852903366\n",
      "Batch 300, Loss: 0.15259017050266266\n",
      "Batch 400, Loss: 0.22376777231693268\n",
      "Batch 500, Loss: 0.1521637737751007\n",
      "Batch 600, Loss: 0.1482619196176529\n",
      "Batch 700, Loss: 0.16508276760578156\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.139791 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.13365283608436584\n",
      "Batch 100, Loss: 0.1933787763118744\n",
      "Batch 200, Loss: 0.1431856006383896\n",
      "Batch 300, Loss: 0.15614107251167297\n",
      "Batch 400, Loss: 0.23169201612472534\n",
      "Batch 500, Loss: 0.14938068389892578\n",
      "Batch 600, Loss: 0.13996048271656036\n",
      "Batch 700, Loss: 0.16262491047382355\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.138610 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.14919671416282654\n",
      "Batch 100, Loss: 0.19935069978237152\n",
      "Batch 200, Loss: 0.17136837542057037\n",
      "Batch 300, Loss: 0.16316896677017212\n",
      "Batch 400, Loss: 0.23312866687774658\n",
      "Batch 500, Loss: 0.15173675119876862\n",
      "Batch 600, Loss: 0.12388981878757477\n",
      "Batch 700, Loss: 0.16959474980831146\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.137827 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.13299547135829926\n",
      "Batch 100, Loss: 0.177232563495636\n",
      "Batch 200, Loss: 0.18907728791236877\n",
      "Batch 300, Loss: 0.1745416224002838\n",
      "Batch 400, Loss: 0.2540596127510071\n",
      "Batch 500, Loss: 0.13454397022724152\n",
      "Batch 600, Loss: 0.14515800774097443\n",
      "Batch 700, Loss: 0.16775353252887726\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.137283 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.12651677429676056\n",
      "Batch 100, Loss: 0.19075118005275726\n",
      "Batch 200, Loss: 0.17157457768917084\n",
      "Batch 300, Loss: 0.1609293520450592\n",
      "Batch 400, Loss: 0.18922829627990723\n",
      "Batch 500, Loss: 0.13005034625530243\n",
      "Batch 600, Loss: 0.11913071572780609\n",
      "Batch 700, Loss: 0.164958193898201\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.136934 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.1167859435081482\n",
      "Batch 100, Loss: 0.17819692194461823\n",
      "Batch 200, Loss: 0.1776585876941681\n",
      "Batch 300, Loss: 0.1701943576335907\n",
      "Batch 400, Loss: 0.24473606050014496\n",
      "Batch 500, Loss: 0.1279517412185669\n",
      "Batch 600, Loss: 0.1257954239845276\n",
      "Batch 700, Loss: 0.16214822232723236\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.135696 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.12555956840515137\n",
      "Batch 100, Loss: 0.17203043401241302\n",
      "Batch 200, Loss: 0.1833806186914444\n",
      "Batch 300, Loss: 0.16451908648014069\n",
      "Batch 400, Loss: 0.23211166262626648\n",
      "Batch 500, Loss: 0.1461029052734375\n",
      "Batch 600, Loss: 0.15335766971111298\n",
      "Batch 700, Loss: 0.16164715588092804\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.135266 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.1376626044511795\n",
      "Batch 100, Loss: 0.19732517004013062\n",
      "Batch 200, Loss: 0.1794610321521759\n",
      "Batch 300, Loss: 0.16143469512462616\n",
      "Batch 400, Loss: 0.21908000111579895\n",
      "Batch 500, Loss: 0.1479579657316208\n",
      "Batch 600, Loss: 0.13501475751399994\n",
      "Batch 700, Loss: 0.16875150799751282\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.134337 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.1300654113292694\n",
      "Batch 100, Loss: 0.1819467842578888\n",
      "Batch 200, Loss: 0.15992295742034912\n",
      "Batch 300, Loss: 0.15614241361618042\n",
      "Batch 400, Loss: 0.2106800377368927\n",
      "Batch 500, Loss: 0.136114701628685\n",
      "Batch 600, Loss: 0.13744059205055237\n",
      "Batch 700, Loss: 0.15698625147342682\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.133492 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.13551068305969238\n",
      "Batch 100, Loss: 0.17929531633853912\n",
      "Batch 200, Loss: 0.177195206284523\n",
      "Batch 300, Loss: 0.17123004794120789\n",
      "Batch 400, Loss: 0.24926011264324188\n",
      "Batch 500, Loss: 0.14400321245193481\n",
      "Batch 600, Loss: 0.1340741664171219\n",
      "Batch 700, Loss: 0.1568296253681183\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.133269 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Batch 0, Loss: 0.15449225902557373\n",
      "Batch 100, Loss: 0.17105093598365784\n",
      "Batch 200, Loss: 0.1819242686033249\n",
      "Batch 300, Loss: 0.15989764034748077\n",
      "Batch 400, Loss: 0.2521609961986542\n",
      "Batch 500, Loss: 0.13828718662261963\n",
      "Batch 600, Loss: 0.14926031231880188\n",
      "Batch 700, Loss: 0.1729828268289566\n",
      "Test Error: \n",
      "Accuracy: 49.7%, Avg loss: 0.132326 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimiser)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78daab-58c5-4711-9225-d9063e4c91e9",
   "metadata": {},
   "source": [
    "If you kept track of your loss/accuracy with a variable you can use the helper function below to plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "071a9dab-dc82-4959-a1a1-dfaec86cad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(loss: List[int], acc: List[int]):\n",
    "    \"\"\"Helper to plot loss and accuracy if you store them in a list\"\"\"\n",
    "    fig, (ax0, ax1) = plt.subplots(1,2,figsize=(16,5))\n",
    "    ax0.plot(acc, 's-')\n",
    "    ax0.set_title(f'Final test accuracy {acc[-1]:.2f}%')\n",
    "    ax0.set_ylabel('Accuracy%')\n",
    "    ax0.set_xlabel('Epochs')\n",
    "    ax1.plot(loss, 's-')\n",
    "    ax1.set_title(f'Final test loss {loss[-1]:.2f}')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c2cd8-b70d-48a3-a727-e321e740433d",
   "metadata": {},
   "source": [
    "Congratulations you have trained a neural network. Most likely the results from this will be less than satisfactory, with it being possilby lower than 50%. Now this is only the start of your journey, every single variable whether your *loss function*, *optimiser (and its learning rate)*, *extracted features*, *model layers*, and more can be changed as to create a better model. This process if called **hyperparameter tuning** and is a large part of machine learning. If you ever want to get better at ML you must be willing to spend the time the improve your model. Therefore the last task is:\n",
    "\n",
    "**Improve your model to have a better accuracy**. Tune respective parameters as to produce the best result possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
