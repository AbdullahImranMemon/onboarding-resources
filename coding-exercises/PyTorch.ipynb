{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d1b74e-6798-4db6-8fcf-e90fd558fee3",
   "metadata": {},
   "source": [
    "# PyTorch & Neural Networks\n",
    "These exercises will be an introduction to Torch and neural networks. So far you should have at least a general idea of what neural networks are. What I now imagine the main question on your mind is how do I begin programming them? While there are many different Python libraries that can be used for the purpose, most of our work is done using PyTorch. PyTorch is a library which is used for machine learning and creating neural networks. This notebook is an exploration of not only how to build a basic neural network but what parts make up PyTorch. This can be spit into 3 main parts:\n",
    "- What is a Tensor, and how is it different from a Numpy array and a Pandas dataframe.\n",
    "- What is a dataloader, and how do we make one.\n",
    "- What is a neural network, and how do modules allow us to implement them.\n",
    "\n",
    "The reason we use PyTorch rather than its rival Tensorflow is that currently the academic community seem to prefer it. This means no matter what topic we are studying their is a large chance its code is written in Torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f197d2-fe3c-448d-8702-2f604fab0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import Tensor\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "from typing import Tuple, List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "type LossFN = Union[Module]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51797708-78b9-4a4b-934f-e79fb4778d0d",
   "metadata": {},
   "source": [
    "## Numpy vs Pandas vs Torch\n",
    "Before we go into the more advanced exercises I wanted to first answer a question that comes up very often. Why are there so many different representations of a list. If you've ever programmed you have probably interacted with a list. A list allows us to store data as well as do vector and matrix operations. In Python this list is dynamically sized, meaning that we can expand it and shrink it with methods like `.append()` and `.remove()`. This permits a degree of flexibility however comes with the drawback that it is slower. This is a problem when we need to do hundreds of operations on lists, which is required within neural networks. So how do we solve this?\n",
    "\n",
    "### Numpy & Numpy Arrays\n",
    "Numpy arrays are a solution to this problem of lists being too slow. Numpy arrays are arrays rather than lists meaning they are one fixed sized. This is the first reason they are significantly more efficient. The second reason is that they are implemented in C rather than Python. C being a low-level language makes most of its operations much faster than Python. The last is parallelism, which is the ability of your computer to do multiple calculations at the same time. Numpy arrays are able to do operations on multiple elements within an array at the same time, resulting in an obvious speedup. The below code illustrates this point by running additions on a list of 100,000 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea653e0-941a-4585-aa19-b17b8b781c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for normal Python: 2.5891454219818115\n",
      "Time for Numpy: 0.004225254058837891\n"
     ]
    }
   ],
   "source": [
    "big_num = 100000\n",
    "\n",
    "# Python's turn\n",
    "start_time = time.time()\n",
    "py_arr = list(range(big_num))\n",
    "for _ in range(400):\n",
    "    for i in range(len(py_arr)):\n",
    "        py_arr[i] += py_arr[i]\n",
    "print(f\"Time for normal Python: {time.time() - start_time}\")\n",
    "\n",
    "# Numpy's turn\n",
    "start_time = time.time()\n",
    "np_arr = np.arange(big_num)\n",
    "for _ in range(400):\n",
    "        np_arr += np_arr # This is faster than using a seperate loop\n",
    "print(f\"Time for Numpy: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d63ee-233c-4eb6-9e1f-d5a8aeca8766",
   "metadata": {},
   "source": [
    "An interesting part of the above code is `np_arr += np_arr`. The reason this works is a lot of Numpy operations are considered *vectorised*. This is just a fancy word for meaning most operators such as +, -, /, and more are done element-wise. This means to optimise your code within Python a good approach is to leverage these Numpy operations to speed up array calculations.\n",
    "\n",
    "Despite this flexibility Numpy arrays do suffer from one main issue, all elements need to be the same type. This means no mixing integers with floats or floats with strings. So what happens if we're reading off of a spreadsheet with multiple forms of data, this is where we need Pandas.\n",
    "\n",
    "### Pandas & Pandas Dataframes\n",
    "Pandas dataframes while not used directly in neural networks provide us an easy way to read and manipulate data. Pandas uses their version of a list/matrix called a dataframe. A dataframe is pretty much just a table. It holds columns with names, and data which can include strings, floats, integers, or even complex objects like dates. This makes them key when dealing with most forms of data.\n",
    "\n",
    "Interestingly despite having multiple different types dataframes aren't as slow as one may think as they are built on top of Numpy arrays. This makes Pandas, while not as fast as numpy, still faster than regular Python.\n",
    "\n",
    "### Torch Tensors\n",
    "While Numpy arrays are amazing for general mathematical and scientific calulations they aren't quite as fast as we'd actually like for neural networks, especially when it comes to backpropagation and optimisation. Therefore, PyTorch has their own array implementation called tensors that are optimised for neural networks. These can easily be converted between formats as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc7fee8-ad64-4eab-97b1-0789e29d1787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([1, 2, 3, 4])\n",
      "With Shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Converting NDArray -> Tensor\n",
    "x = np.array([1, 2, 3, 4])\n",
    "x_tensor = torch.tensor(x)\n",
    "\n",
    "# Converting List -> Tensor\n",
    "z = [4, 3, 2, 1]\n",
    "z_tensor = torch.tensor(z)\n",
    "\n",
    "# Similar to numpy shape data can be found here as well\n",
    "print(f\"Tensor: {x_tensor}\\nWith Shape: {x_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72895c3-5fce-4f62-9038-db9d64c81ba4",
   "metadata": {},
   "source": [
    "The killer feature of Tensors that makes them different from Numpy's arrays is the ability to set the device its calculations will be done on. To undestand why this is important we first must discuss the purpose of a GPU.\n",
    "\n",
    "Every computer uses a central processing unit CPU to calculate values and run code. This device is able to do millions of computations every second making operations such as most of the code in this notebook possible however it suffers from 2 big flaws when it comes to computing neural networks. The first is that the CPU is busy running lots of different things to keep your computer working for you. The next is that it is limited by the amount of processing units it has. This low amount makes it slow at doing vector operations such as those found within neural networks. So rather than using a CPU  we can use a device dedicated to doing very small fast vector operations. In comes the graphics processing unit GPU. Invented in 1999 the GPU was first used to improve how fast graphics on a computer were rendered. This is as the rendering of graphics such as those found within computer games requires many vector projection operations. Rather than having dozens of processing units that are really powerful, a GPU has thousands of really weak processing units. These differences make it much more efficient for us to use a GPU rather than our CPU to train neural networks.\n",
    "\n",
    "The below code can be used to find what device you can do your calculations on. This can either be your GPU (called CUDA), MPS (for apple computers), or your CPU. This is then followed with an example of putting a tensor onto the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc09745-bfb9-4c2d-bab9-ca1725ec3bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find device being used\n",
    "device = (\n",
    "    \"cuda\" # For NVIDIA GPUs\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" # For Apple devies\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device.upper()}\")\n",
    "\n",
    "# Place tensor on device\n",
    "x_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a410098f-2cca-4f8a-be7e-9d0a406bdb65",
   "metadata": {},
   "source": [
    "## Dataset & Dataloader\n",
    "The next part of Tensor flow is the Dataset and Dataloader. From what we have already discussed we know a dataset is a collection of data that can be used to train a ML model. The last question sheet saw us iterating through this data as to find a line of best fit. While this solution works, PyTorch provides a way to do this that is a little nicer. Enter PyTorch's `Dataset` abstract base class. \n",
    "\n",
    "A `Dataset` in PyTorch is a class which is can be used to easily iterate through data, it just requires 3 methods to be implemented:\n",
    "- `__init__`, which initialises the dataset usually by loading the data.\n",
    "- `__len__`, which gets the total length of the dataset.\n",
    "- `__getitem__`, which gets a bit of data from the dataset, returning it, usually as a tensor.\n",
    "\n",
    "To give you an idea of what one looks like below is an example for a basic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09d0e6f-c686-4f4a-b1d4-f5d82e80581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Inputs: tensor([[1., 0.]])\n",
      "Labels: tensor([[1.]])\n",
      "Batch 2\n",
      "Inputs: tensor([[0., 1.]])\n",
      "Labels: tensor([[1.]])\n",
      "Batch 3\n",
      "Inputs: tensor([[1., 1.]])\n",
      "Labels: tensor([[0.]])\n",
      "Batch 4\n",
      "Inputs: tensor([[0., 0.]])\n",
      "Labels: tensor([[0.]])\n"
     ]
    }
   ],
   "source": [
    "# This a is common toy example within neural networks\n",
    "# In this dictionary the key (bool, bool) is the data,\n",
    "# and the item bool is the label. (False, False) has label False, etc.\n",
    "xor_data = {(False, False): False, (False, True): True, (True, False): True, (True, True): False}\n",
    "\n",
    "class XORDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        \"\"\"Initiliase the XOR problem's dataset\"\"\"\n",
    "        # Note normally you would start by reading a csv file\n",
    "        # or getting the locations of pictures.\n",
    "        # Do some modifications to turn it into a tensor\n",
    "        inputs = [list(k) for k in data.keys()]\n",
    "        labels = [v for v in data.values()]\n",
    "\n",
    "        # Save these tensors so that we can use them later\n",
    "        # variable names don't matter, just make them clear\n",
    "        self.input = torch.tensor(inputs, dtype=torch.float32)\n",
    "        self.label = torch.tensor(labels, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Get length of our dataset\"\"\"\n",
    "        # Simply return length of our data\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"Get an item from idx\"\"\"\n",
    "        x = self.input[idx] # get data from inputs\n",
    "        y = self.label[idx] # get data from label\n",
    "        return x, y # return them as a tuple so we can unpack\n",
    "\n",
    "xor_dataset = XORDataset(xor_data)\n",
    "xor_dataloader = DataLoader(xor_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Print dataset\n",
    "for batch_idx, (x, y) in enumerate(xor_dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}\\nInputs: {x}\\nLabels: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ddd2e0-ce5d-42d2-bc74-63928e7a4e49",
   "metadata": {},
   "source": [
    "Once you have created a dataset you can now wrap it in PyTorch's `Dataloader`. A dataloader allows you to iterate easily through a dataset while also giving you the ability to change how it gives you data. In the code above we simply wrap the dataset in it and give it a batch_size and set it to shuffle. \n",
    "- `batch_size` allows you to load multiple inputs and labels at the same time. This can speed up training time as it allows you to do more calculations at once. Keep this in mind when creating models.\n",
    "- `shuffle` randomises the order of the dataset, this should be done as it mitigates overfitting.\n",
    "\n",
    "Once we've done this we are ready to train our model. It should output a tensor representing our input, and its associated label. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eac340-0fec-40c1-bfee-bbc03bddd9a3",
   "metadata": {},
   "source": [
    "### **(Question 1)** Create a dataset for insurance\n",
    "Last lesson we created a basic machine learning model from the insurance dataset. This dataset attempts to predict the cost of health-care given a few details. We want to wrap the dataset in PyTorch's dataset. An important part is to make sure that it returns items as tensors. Below is a reminder of some code we used last workshop which may help. This uses numpy so be sure to translate this code into tensors?\n",
    "```python\n",
    "data = pd.read_csv('data/insurance.csv')\n",
    "x = data[\"age\"].to_numpy()\n",
    "y = data[\"charges\"].to_numpy()\n",
    "```\n",
    "\n",
    "**Create a basic dataset for insurance that takes as input a path during initialisation and outputs a tensor representing the age of an individual. This should then be turned into a DataLoader for the dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74a08dda-f858-4237-8f1e-559fafb5598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsuranceDataset(Dataset):\n",
    "    def __init__(self, path: str):\n",
    "        \"\"\"Read the dataset and get the input as a tensor\"\"\"\n",
    "        pass\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Get the amount of insurance entries\"\"\"\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"Get an item representing the \"\"\"\n",
    "        pass\n",
    "        \n",
    "path = \"data/insurance.csv\"\n",
    "insurance_data = None # Fill in this to create the dataset\n",
    "insurance_loader = None # Fill in this to turn the dataset to a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74380dd9-8642-4e6b-90c8-303083102078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[28.0000,  0.0000, 23.8000,  2.0000,  0.0000]])\n",
      "Result: tensor([3847.6741])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (x, y) in enumerate(insurance_loader):\n",
    "    print(f\"Inputs: {x}\\nResult: {y}\") # Print first results\n",
    "    break # Don't worry if not exactly the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff88e10a-f7ef-4535-9935-4c89c9cd3a33",
   "metadata": {},
   "source": [
    "### **(Question 2)** Expand the dataloader\n",
    "Before we move on lets expand our model further. Currently we are only considering age but with tensors we can consider a list of inputs rather than one. This means that we can easily expand our input to have more features, which will improve our model when we decide to train it.\n",
    "\n",
    "**Expand your dataloader to include bmi and amount of children**. When you run the above code it should output a tensor with three values as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b5ce676-3c97-42d8-b4e2-2ced6df301d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to remind you of the data we are looking at\n",
    "pd.read_csv('data/insurance.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c4d91c2-d3c4-45fb-af7e-56e6169da523",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = len(next(iter(insurance_loader))[0][0])\n",
    "if d % 3 != 0 and d % 5 != 0:\n",
    "    print(\"Unsucessfully added bmi and children\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a861a-08c7-48bc-b5f3-b29fbf132ea5",
   "metadata": {},
   "source": [
    "### **(Question 3)** Encoding data in the dataloader\n",
    "Now that we are considering more inputs our model should improve. Despite this we are missing whether they are a smoker and their gender. If we were to just put them into the tensor we would run into an issue though, the data needs to be a float, which both fields aren't as they are strings (called object by Pandas). To fix this we need to encode the data into a format we can use. This can be done through two main approaches:\n",
    "- Label encoding, where every category is encoded as an integer. This can be done automatically or done by using a relevant number, such as the amount of appearences. For example a smoker could be classified as `1` while a non-smoker could be classified as `0`\n",
    "- On hot encoding, where each category is represented as $n$ Boolean columns, where $n$ is the amount of categories. Below is an example with `RainToday`.\n",
    "\n",
    "| ID | Smoker |\n",
    "| ---- | ----- |\n",
    "| 0 | Yes |\n",
    "| 1 | No |\n",
    "\n",
    "| ID | Yes | No |\n",
    "| --- | ---- | ----- |\n",
    "| 0 | 1 | 0 |\n",
    "| 1 | 0 | 1 |\n",
    "\n",
    "**Expand your dataset to have smoker and sex as a part of the tensor by encoding them as floats**.\n",
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "    Some functions which may be useful depending on the approach used are:\n",
    "    <ul>\n",
    "        <li><code>pd.get_dummies</code></li>\n",
    "        <li><code>dataset[...].map({})</code></li>\n",
    "        <li><code>OneHotEncoder</code> from sklearn</li>\n",
    "        <li><code>LabelEncoder</code> from sklearn</li>\n",
    "    </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f7182c-d540-42e0-8160-b3be01bd3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = len(next(iter(insurance_loader))[0][0])\n",
    "if d % 5 and d % 6 :\n",
    "    print(\"Unsucessfully added smoker and sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87a21ce-d034-4229-a559-8782ed8de8c8",
   "metadata": {},
   "source": [
    "### **(Question 4)** Split the Dataset\n",
    "Currently the dataset only consists of one main dataset which iterates through all entries. This causes problems however as after we train the model and give it new data it may be predicting what our data says rather than predicting it from the trends. To avoid this we usually split the dataset into 2 halves, the training set and the testing set.\n",
    "- Training set, consists of most of the data usually 75% and above, and is used to train our ML model.\n",
    "- Testing set, consists of the rest of the data, and is used to test our dataset at increments to make sure it still accurately predicts cases where it encounters new data.\n",
    "\n",
    "By splitting the dataset we are able to reduce overfitting and see how our accurate our model is. So before we go about training we first need to go about splitting it. This is done by first calculating the training sizes and test sizes. Before then using `random_split` to split the data. After we have done this we can create new dataloaders for both training and testing data.\n",
    "\n",
    "**Split the dataset as to create a new training and testing dataloader, where training consists of 80% of the data**. Do it once again in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799cfb8-a553-46a5-9fd3-aeff82a19d3f",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "### XOR Problem\n",
    "The XOR problem is a toy problem where our goal is to learn XOR. But first what is XOR? Exclusive Or (XOR) in programming is a logic gate found within many computers. In Python we can call it with `^`, for example `0 ^ 1` will be `1` in Python. For the purposes of this task it is just a function (or rule) that we want to be able to find with our neural network. Below is a truth table for XOR, this will be our dataset.\n",
    "\n",
    "| X | Y | X^Y |\n",
    "| - | - | --- |\n",
    "| 0 | 0 |  0  |\n",
    "| 0 | 1 |  1  |\n",
    "| 1 | 0 |  1  |\n",
    "| 1 | 1 |  0  |\n",
    "\n",
    "If you remember before we actually turned this into a dataset called `XORDataset`. As we have already done this we can move onto the next step, training our model. But first what is a module in PyTorch?\n",
    "\n",
    "### Modules\n",
    "PyTorch represents all neural networks with the `Module` abstract base class. This class requires two functions to be implemented.\n",
    "- `__init__` which stores the layers of the network and its activation functions.\n",
    "- `forward` which takes the input tensor and does the forward pass by running its layers and associated activation functions as to turn the input tensor into a prediction. Each layer is required to start and end with the same amount of neurons as the last.\n",
    "\n",
    "Below is an example network for our problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79cf7b25-311f-4ec2-84e7-2f788efb4f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple feedforward neural network\n",
    "class XORNetwork(Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Use this to store layers and activation functions\"\"\"\n",
    "        super().__init__() # This is required\n",
    "        # Layers\n",
    "        self.hidden = nn.Linear(2, 2) # Takes two inputs returns two\n",
    "        self.output = nn.Linear(2, 1) # Takes two as layer above is the same.\n",
    "        # Activation Function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Run the forward pass with this data\"\"\"\n",
    "        # Forward pass: input -> hidden layer -> sigmoid -> output layer -> sigmoid\n",
    "        x = self.sigmoid(self.hidden(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909ba06-5442-4bcb-9a9d-f49ec97af261",
   "metadata": {},
   "source": [
    "What makes this interesting is that we can just as easily reuse this module in the future by simply putting it in our network. For example below we expand our network by adding an extra layer. This ability to compose networks in this way allows use to improve architectures made by others. It also means that `Linear`, and `Sigmoid` are also both modules that can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "698af6ae-4d32-486c-b0b8-db612ba6aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigXORNetwork(Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Use this to store layers and activation functions\"\"\"\n",
    "        super().__init__() # This is required\n",
    "        self.hidden = nn.Linear(2, 2)\n",
    "        self.output = XORNetwork()\n",
    "        # Activation Function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Run the forward pass with this data\"\"\"\n",
    "        # Forward pass: input -> hidden layer -> hidden layer -> sigmoid -> output layer -> sigmoid\n",
    "        x = self.sigmoid(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f42bab-1de4-4500-82e1-14f6d8379c2d",
   "metadata": {},
   "source": [
    "### Training & Testing\n",
    "Once we have created our model we can start to train it and testing it. Before we start this process there are 3 things that need to be done, first create our model by calling its class, then define a loss function. Think back to linear regression at any given point we first need to calculate how well we are doing at any given step. This starts with our loss function. The last step is defining our optimiser. This is once again similar to linear regression, but rather than creating our own version we can use PyTorch's. Once we have done these three things we can continue.\n",
    "\n",
    "The training stage follows the steps:\n",
    "1. Set model to `model.train()`, this ensures any changes to the weights and biases are done.\n",
    "2. Iterate through batches within the data loader.\n",
    "3. Assign the iterated values to a device `X, y = X.to(device), y.to(device)`. This is important if we want our model to train fast.\n",
    "4. Compute the predicted error (*forward pass*). This is done by running the model `model(X)`, before calculating the loss. Think linear regression.\n",
    "5. Run backpropogation (*back pass*). This is done by calculating the gradients for each neuron. Once we have these gradients we can change the weights and biases by stepping the optimiser. After this we ensure all gradients are set to zero for the next iteration.\n",
    "6. Optionally print the loss.\n",
    "\n",
    "Below is an example of this on our XOR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9427f732-e4e8-4504-b360-9240faaaf579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/10000, Loss: 0.711939811706543\n",
      "Epoch 2000/10000, Loss: 0.7355634570121765\n",
      "Epoch 3000/10000, Loss: 1.085893154144287\n",
      "Epoch 4000/10000, Loss: 0.0374777726829052\n",
      "Epoch 5000/10000, Loss: 0.022829413414001465\n",
      "Epoch 6000/10000, Loss: 0.008630456402897835\n",
      "Epoch 7000/10000, Loss: 0.00952976755797863\n",
      "Epoch 8000/10000, Loss: 0.007342242635786533\n",
      "Epoch 9000/10000, Loss: 0.0036172533873468637\n",
      "Epoch 10000/10000, Loss: 0.0030730527359992266\n",
      "\n",
      "Given input: 0, 0\n",
      "Result: 0.003065339755266905\n"
     ]
    }
   ],
   "source": [
    "# Create network\n",
    "model = XORNetwork().to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "loss_fn = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimiser = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10000\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for X, y in xor_dataloader:\n",
    "        # Forward pass\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward() # Calculate gradients\n",
    "        optimiser.step() # Change weights and biases as a result of gradients\n",
    "        optimiser.zero_grad() # Set gradients to zero\n",
    "        \n",
    "    # Print loss every 1000 epochs\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Test the model, this is usually done multiple times\n",
    "model.eval()\n",
    "with torch.no_grad():  # No need to track gradients for testing\n",
    "    inputs = torch.tensor([0.0, 0.0])\n",
    "    test_output = model(inputs)\n",
    "    print(f\"\\nGiven input: 0, 0\\nResult: {test_output[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1cbe17-e0e7-45fa-8b87-5bbbdcc53e7a",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "Now that we have a basic idea of how a neural network works we can expand these concepts to CNNs by letting you do a basic example. \n",
    "\n",
    "### Hand-Writing Recognition\n",
    "Hand-writing recognition is a classic problem neural network that involves recognising numbers from people's hand-writing. For example if we see a drawn 6 we want our model to output a 6. In fact this problem is so common we can save some time by downloading the dataset from Torch. Looking at the call below we see this is fairly similar to calling our own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a901539-2830-42eb-ab18-38318bd8798c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remember this place for later\n",
    "preprocessing = transforms.ToTensor()\n",
    "\n",
    "# Download training data from torch\n",
    "training_data = datasets.QMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocessing,\n",
    ")\n",
    "\n",
    "# Download test data from torch\n",
    "test_data = datasets.QMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# Display the data\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890bcbf-9ea4-4ad0-aadf-f1feb3547711",
   "metadata": {},
   "source": [
    "### **(Question 5)** Model\n",
    "Now that we have seen our data our first goal is to make a basic neural network for our dataset. This involves creating a module like before. An important part to keep in mind is that we can't accept this data in its 2d form. As such our first step is to `flatten` the data. Once this is done we can move onto building out our layers and activation functions.\n",
    "\n",
    "**Build a basic neural network**. Focus on it working at first, we can improve this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cbe2053-1012-4dfb-a851-a3496ed79cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMNISTNet(Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Layers for our handwritting model\"\"\"\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Give handwritting prediction\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829aac50-5296-4a36-a7b5-3a04e1086491",
   "metadata": {},
   "source": [
    "### **(Question 6)** Training\n",
    "From our previous example we saw how you would go about training and testing a model. Our first order of business is to make it less messy. A nice way of doing this is by declaring seperate function for `train` and `test`. This will allow us to take parameters such as the dataloader, model, loss function, and optimiser as arguments which can be useful if we want to test multiple different things. Similar to last time you need to iterate through your data, forward pass, and back pass. Look at the previous example if you need some help.\n",
    "\n",
    "**Create a function that takes in a dataloder, a model, a loss function, an optimiser, and trains the network**. This function will be called every epoch so don't worry about making a loop for the amount of epochs here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed934eca-a634-4c8f-a1c2-738addcbd068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader: DataLoader, model: Module, loss_fn: LossFN, optimiser: Optimizer):\n",
    "    \"\"\"Train the model using the training set\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997e9520-8566-49c9-9fed-b030df7ea9d2",
   "metadata": {},
   "source": [
    "### **(Question 7)** Testing\n",
    "Once we have finished running training we can now test the model against our testing dataset. This will provide us with a look into its current loss and accuracy. This will be more different to our previous example as we now want to calculate both the loss and accuracy after running the test function. This will be done similar to training however rather than backpropagating we store our loss and accuracy, allowing us to print them at the end of the epoch.\n",
    "\n",
    "**Create a test function that computes the loss and accuracy for data in the test dataloader**. Make sure to divide the loss and accuracy at the end. To predict our accuracy you can calculate from a prediction `pred`, and a label `y` the result to be:\n",
    "```python\n",
    "res = pred.argmax(dim=1, keepdim=True)\n",
    "acc += res.eq(y.view_as(res)).sum().item()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2880b4f3-75e3-4097-811e-e55b9d2cabdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader: DataLoader, model: Module, loss_fn: LossFN):\n",
    "    \"\"\"Test the model to see how it compares against the test portion of the dataset\"\"\"\n",
    "    n = len(dataloader.dataset)\n",
    "    n_batch = len(dataloader)\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    # Fill this in, what do we want to do next? Perhaps iterate through data\n",
    "    # to calculate accuracy?\n",
    "    loss /= n_batch\n",
    "    acc /= n\n",
    "    print(f\"Accuracy: {(acc):>0.1f}%, Avg loss: {loss:>8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40b09f-88de-4664-9a9e-1d6e821bb60f",
   "metadata": {},
   "source": [
    "### **(Question 8)** Loss & Optimiser\n",
    "The final step before we can run our model is choosing a loss function and optimiser. For now you can just enter the previously used ones, however be sure to change these later to see how the accuracy changes given these variables.\n",
    "\n",
    "**Choose a loss function and optimiser**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32cdfe2e-e874-4242-94ad-60452e7ef546",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QMNISTNet()\n",
    "loss_fn = None # Fill this in\n",
    "optimiser = None # Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65158471-b6ec-44db-85c0-1e32d32d6ada",
   "metadata": {},
   "source": [
    "### Results\n",
    "Given that you have done the previous steps correctly you should be able to run the below code as to train the model for yourself. This code runs 10 epochs before finishing. If you are willing to spend more time training be sure to modify the amount of epochs. Don't expect this to be super accurate on the first try as we are still learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "691862a0-efea-436c-947a-e34fac220bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Accuracy: 0.4%, Avg loss: 2.260161\n",
      "Epoch 2/10: Accuracy: 0.5%, Avg loss: 2.204112\n",
      "Epoch 3/10: Accuracy: 0.6%, Avg loss: 2.111032\n",
      "Epoch 4/10: Accuracy: 0.6%, Avg loss: 1.950027\n",
      "Epoch 5/10: Accuracy: 0.7%, Avg loss: 1.697428\n",
      "Epoch 6/10: Accuracy: 0.7%, Avg loss: 1.384210\n",
      "Epoch 7/10: Accuracy: 0.8%, Avg loss: 1.106342\n",
      "Epoch 8/10: Accuracy: 0.8%, Avg loss: 0.912434\n",
      "Epoch 9/10: Accuracy: 0.8%, Avg loss: 0.783257\n",
      "Epoch 10/10: Accuracy: 0.8%, Avg loss: 0.693901\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}/{epochs}: \", end=\"\")\n",
    "    train(train_dataloader, model, loss_fn, optimiser)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9eace2-cbd5-4446-a801-12a96f494e7c",
   "metadata": {},
   "source": [
    "### **(Question 9)** Preprocessing\n",
    "Now that you have done all the previous steps you should have a model that ranges from ok to pretty decent. However, one thing that may be holding you back is the lack of preprocessing. So what is preprocessing and how can we use it?\n",
    "\n",
    "Preprocessing is the idea of modifying your data before you use it in a network. An example you have already done is back when you encoded male and female as 0 and 1. Another example can also be filling in data points that you can't find with a mean/median. With images this approach is different, rather we want to normalise the data, do rotations, change pixels and more.\n",
    "\n",
    "The goal with preprocessing in general is to make our dataset more generalisable so that it not only can have an easier time training, but be able to be ran on new data and get a more accurate result. Below is an example of how we can change the preprocessing steps in our code. This creates a pipeline that starts by rotating before turning our input to a tensor.\n",
    "```python\n",
    "preprocessing = transforms.Compose([\n",
    "    transforms.RandomRotation((0, 360)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "```\n",
    "\n",
    "**Modify the preprocessing step to be more advanced**. This step can be found in the place you downloaded the dataset. Be sure to experiment with different things not just rotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a81cdef-808b-43bd-90ba-632383ae5a8a",
   "metadata": {},
   "source": [
    "### **(Question 10)** Improve your Model\n",
    "Congratulations you have trained a neural network. Most likely the results from this will be less than satisfactory, with it being possibly lower than 50%. Now this is only the start of your journey, every single variable whether your *loss function*, *optimiser (and its learning rate)*, *extracted features*, *model layers*, and more can be changed as to create a better model. This process if called **hyperparameter tuning** and is a large part of machine learning. If you ever want to get better at ML you must be willing to spend the time the improve your model. Therefore the last task is:\n",
    "\n",
    "**Improve your model to have a better accuracy**. Tune respective parameters as to produce the best result possible. This will require some research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c07b89b3-bbdb-4c17-a546-94aa04aa6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to pretify your loss and accuracy graphs here:\n",
    "def plot_loss_acc(loss: List[int], acc: List[int]):\n",
    "    \"\"\"Helper to plot loss and accuracy if you store them in a list\"\"\"\n",
    "    fig, (ax0, ax1) = plt.subplots(1,2,figsize=(16,5))\n",
    "    ax0.plot(acc, 's-')\n",
    "    ax0.set_title(f'Final test accuracy {acc[-1]:.2f}%')\n",
    "    ax0.set_ylabel('Accuracy%')\n",
    "    ax0.set_xlabel('Epochs')\n",
    "    ax1.plot(loss, 's-')\n",
    "    ax1.set_title(f'Final test loss {loss[-1]:.2f}')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epochs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
