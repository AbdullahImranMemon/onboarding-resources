<h1>~~~ Introduction to Machine Learning ~~~</h1>

![figure_13.jpg](attachment:figure_13.jpg)

<div style="background-color: #5c5c5c; color: white; padding: 10px; border-radius: 5px;">

<h2>Table of Contents: </h2>

1. Welcome to Monash AIM    

2. Preface   
    a. Vectors and matrices  
    b. Matrix addition, subtraction, and scalar multiplication  
    c. Dot products  
    d. Matrix multiplication    
    e. Rules for matrix addition and multiplication  
    f. Graph minimums/maximums and gradients  
    g. Derivatives  
    h. SIGMA notation  
    i. PI notation  
    j. Videos  

3. Machine learning   
   a. What is AI and Machine Learning?  
   b. Types of learning: supervised, unsupervised, and reinforcement learning  
   c. Types of models: linear regression vs classification  
   d. Linear regression (cost and loss functions)  
   e. Underfitting and overfitting  
   f. Logistic regression   

4. Recap and future steps
<h2> <span style="color: #f28f2c;"> ~~~ Part 1. Welcome to Monash AIM ~~~ </span> </h2>
The following series of notebooks were created by the [Monash Analysis of Images in Medicine (Monash AIM)](https://monashaim.github.io/) student team with the purpose of on-boarding new members. The goal is to teach both the theory and application behind neural networks for medical imaging so that students can participate in projects. This is covered across 5 notebooks which teach:

<table>
    <tr style="background-color: #a17337">
        <th> No. </th>
        <th> Notebook title </th>
        <th> Description </th>
    </tr>
    <tr>
        <td> 0 </td>
        <td> Software </td>
        <td> a tutorial on the software Monash AIM will be using throughout its projects. </td>
    </tr>
    <tr>
        <td> 1 </td>
        <td> Introduction to machine learning </td>
        <td> the basics of modelling data in preparation for learning neural networks. </td>
    </tr>
    <tr>
        <td> 2 </td>
        <td> Introduction to neural networks </td>
        <td> the theory behind neural networks including neurons, activation functions, and backpropagation. </td>
    </tr>
    <tr>
        <td> 3 </td>
        <td> Application of convolutional neural networks </td>
        <td> where previous knowledge is applied to learn about the structure of CNNs with 2 medical imaging case studies. </td>
    </tr>
    <tr>
        <td> 4 </td>
        <td> Medical imaging types </td>
        <td> the types of imaging that are common within the field. </td>
    </tr>
</table>

I'd like to thank the various contributors to the on-boarding resources as they made all of this possible.

<b>Note:</b> if this is the first notebook you are reading, we recommend going back to <i>Notebook 0 - Software</i> which provides instructions on how and which pieces of software to install before you are able to contribute in Monash AIM's projects.    
<h2> <span style="color: #f28f2c;"> ~~~ Part 2. Preface ~~~ </span> </h2>

Section 2 of this notebook goes through basic linear algebra and calculus related concepts that you may have already seen in highschool mathematics or in previous units at Monash. Ultimately, many of these operations will be automated by functions and mathematical operations provided by Python. Therefore, as long as you have a general understanding of the processes that are occurring in the background, this will improve your ability to understand and train more efficient neural networks. A video has been provided in section 2j which runs through this preface section and provides some basic examples to follow along. 
<h4 id="2a_cell"> <span style="color: #f28f2c;"> 2a. Vectors and matrices </span> </h4>

<b>What is a vector?</b>  
A vector is <b>a list of real numbers</b>. Variables representing vectors are depicted as either: 
- Characters with an arrow on top ($\vec{v}$) when handwritten 
- Bold characters ($\mathbf{v}$) when typed
  
For example: 

$$
\vec{v} = \mathbf{v} = \begin{pmatrix}
v_1 \\
v_2 \\
v_3
\end{pmatrix}
$$

Geometrically, vectors can be interpreted as either: 
- a point in space

![image.png](attachment:image.png)

- or arrows with both a magnitude and a direction (but no position)

![image-2.png](attachment:image-2.png)

In the above examples: 
- $\mathbf{a}$ is 2-dimensional because it is a list of two numbers and it lives in a plane (only a horizontal and vertical axes). 
- $\mathbf{b}$ is a 3-dimensional vector with a list of three numbers and it lives in a 3D space (x, y, and z-axes). 
<b>What is a matrix?</b>  

A matrix is a <b>rectangular array of real numbers</b> with $m$ rows and $n$ columns. $m$ and $n$ ($m \times n$) defines the <b>dimensions</b> of the matrix and the product of these two values determines how many elements/entries are in the matrix. To clarify the difference between a matrix and vector, a vector is essentially a special type of matrix that happens to have only one row or column as it is a list of numbers (not an array like a matrix). We refer to an entry in the matrix in relation to the row and column it is located in i.e., element $\mathbf{A_{m,n}}$ is the entry in the $\text{m-th}$ row, $\text{nth}$ column:  

$$
\mathbf{A} = \begin{pmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\ 
a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\ 
\vdots & \vdots & \ddots & \vdots \\
a_{m,1} & a_{m,2} & \cdots & a_{m,n} 
\end{pmatrix}
$$

Examples: 
1. $\mathbf{A} = \begin{pmatrix} 0 & 1 \\ 2 & 3 \end{pmatrix}$ is a $2 \times 2$ matrix, where the entry in the 2nd row 1st column is given by $\mathbf{A_{2,1}} = 2$ 
2. $\mathbf{B} = \begin{pmatrix} -3 & 62 & -7 \\ 5 & 11 & -1 \end{pmatrix}$ is a $2 \times 3$ matrix, where the entry in the 2nd row 2nd column is given by $\mathbf{B_{2,2}} = 11$ 
<b>Identity matrix</b>

An identity matrix is a special type of matrix that is distinct for a number of reasons: 
- It is a square matrix which means there are an equal number of rows and columns i.e., the dimensions of an identity matrix are $n \times n$
- The entries in $I_{n,n} = 1$ and 0 everywhere else (pictorially, you will notice that there is a diagonal of 1's located in the leading diagonal)

Identity matrices are written as $I_n$ where $n$ refers to the dimension of the identity matrix. For example: 
- $I_2$ indicates that we have a $2 \times 2$ identity matrix: 
$$
I_2 = \begin{pmatrix}
1 & 0 \\ 0 & 1 
\end{pmatrix}
$$

- whereas $I_3$ indicates that we have a $3 \times 3$ identity matrix: 
$$
I_3 = \begin{pmatrix}
1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1  
\end{pmatrix}
$$

<h4 id="2b_cell"> <span style="color: #f28f2c;"> 2b. Matrix addition, subtraction, and scalar multiplication </span> </h4>

There are four main arithmetic operations when working with matrices: 
- Addition or subtraction
- Scalar multiplication
- Matrix multiplication
- Dot product (also known as 'inner product' or 'scalar product')
<b>Addition and subtraction</b>  

Vectors (or matrices) can only be added given that two vectors/matrices have the same dimension. In other words, we can only add $\mathbf{A_{m,n}}$ and $\mathbf{B_{r,c}}$ together if $m=r$ and $n=c$. This addition is performed 'element-wise' where we grab the entries at the same positions in each vector/matrix and add them together. This produces another vector/matrix with the same dimensions. 

For example, let $\mathbf{u}$ and $\mathbf{v}$ be two vectors: 

$$\mathbf{u} = \begin{pmatrix}
u_1 \\
u_2 \\
\vdots \\
u_m
\end{pmatrix}, \quad
\mathbf{v} = \begin{pmatrix}
v_1 \\
v_2 \\
\vdots \\
v_m
\end{pmatrix}
$$

The sum of the vectors $\mathbf{u}$ and $\mathbf{v}$ is given by: 

$$
\mathbf{u} + \mathbf{v} = \begin{pmatrix}
u_1 + v_1 \\
u_2 + v_2 \\ 
\vdots \\ 
u_m + v_m 
\end{pmatrix}
$$

Subtraction essentially works the same way as addition except we are subtracting the entries from each vector/matrix that are in the same position instead of adding them. 
<b>Scalar multiplication</b>

*A scalar is a quantity or magnitude without any directional component*. The product of a scalar, $c$ and matrix $\mathbf{A}$ is the element-wise multiplication of $\mathbf{A}$ by $c$. Essentially, it is a number that we multiply all elements of a matrix by.

For example, scalar multiplcation of a vector $\mathbf{v}$ by a scalar $a = 3$ is given by: 

$$
a\mathbf{v} = 3 \begin{pmatrix}
2 & 0 \\ -9 & 3
\end{pmatrix} = \begin{pmatrix}
3 \times 2 & 3 \times 0 \\ 3 \times -9 & 3 \times 3 
\end{pmatrix} = \begin{pmatrix}
6 & 0 \\ -27 & 9
\end{pmatrix}
$$
<h4 id="2c_cell"> <span style="color: #f28f2c;"> 2c. Dot products </span> </h4>

<b>Dot products</b>

Taking the dot product of two vectors (also known as the inner product or scalar product) involves taking two vectors $\mathbf{v}$ and $\mathbf{w}$, multiplying together the values of the entries across each vector that are in the same position, and taking the sum of all the products. This means that we can only take the dot product of two vectors that have the exact same dimensions! This also applies for matrices as well. 

$$
\mathbf{v} \cdot \mathbf{w} = \begin{pmatrix} 
v_1 \\ v_2 \\ \vdots \\ v_d \end{pmatrix} \cdot \begin{pmatrix} 
w_1 \\ w_2 \\ \vdots \\ w_d \end{pmatrix} = 
v_1w_1 + v_2w_2 + ... + v_dw_d
$$

We can also represent this geometrically (only for vectors and not matrices) as: 

$$
\mathbf{v} \cdot \mathbf{w} = |v| \cdot |w| \cdot cos(\theta) 
$$

<div align='center'>

![image.png](attachment:image.png)

<b>Different representations of the dot product (graphical vs algebraic).</b>

</div>

where:  
- $|v|$ is the magnitude of vector $\mathbf{v}$,  
- $|w|$ is the magnitude of vector $\mathbf{w}$,  
- $\theta$ is the angle between the two vectors 

Notice that the result of the dot product is a number (not a vector or matrix), reflecting how closely two vectors align in terms of the directions they point. If you remember the shape of a cosine graph: 
- when $\theta = 0$, $cos(0) = 1$ which implies that the vectors are pointing in the same direction (parallel) 
- as $\theta$ increases, the value of $cos(\theta)$ decreases as the vectors become less and less parallel until $\theta = 90$ degrees where $cos(90)=0$ and the two vectors are perpendicular
<h4 id="2d_cell"> <span style="color: #f28f2c;"> 2d. Matrix Multiplication </span> </h4>

<b>When can we multiply two matrices together?</b>

Matrix multiplication is almost a completely different operation from scalar multiplcation so try not to get the two confused. Before we go on to perform matrix multiplication, we need to determine if two matrices are actually able to undergo matrix multiplication in the first place! 

Two matrices are able to matrix multiply if and only if they have compatible dimensions. Luckily, we can not only determine whether two matrices are able to undergo matrix multiplcation, but also determine the dimensions of the final product by writing out the dimensions. 

For example, let's assume that we have two matrices $\mathbf{A}$ and $\mathbf{B}$ with dimensions $n \times m$ and $m \times r$ respectively and we want to calculate $\mathbf{A} \times \mathbf{B} = \mathbf{C}$: 

![image.png](attachment:image.png)

To make things easier, write out the dimensions of the two matrices right next to each other in the order of multiplication e.g., $n \times m$ and $m \times r$
- If the two inner dimensions are equal to each other ($m=m$ in this case), we can multiply $\mathbf{A}$ and $\mathbf{B}$ together 
- The dimensions of the product $\mathbf{C}$ are given by the two outer dimensions i.e., $2 \times 1$ 
- The dimensions have been labelled in the figure above with specific colours for easier navigation

Notice in this example that we were performing the matrix multiplication $\mathbf{A} \times \mathbf{B}$. If we were to swap the two matrices around and perform $\mathbf{B} \times \mathbf{A}$, the dimensions requirement is not satisfied anymore ($n \neq r$ as $2 \neq 1$) and consequently, we cannot calculate $\mathbf{B} \times \mathbf{A}$. 

<b>How to multiply two matrices together?</b>

Now that we know whether we can continue with matrix multiplication, as well as the dimensions of the final product, how do we actually perform matrix multiplication? We define the product $A \times B$ of two matrices by "pouring" the rows of $\mathbf{A}$ into the columns of $\mathbf{B}$. Alternatively, you may have heard of 'running and diving' instead of 'pouring' as we multiply the horizontal rows (running) of the first matrix by the vertical columns (diving) of the second matrix: 

![image-2.png](attachment:image-2.png)

- When we are trying to determine the $C_{i,j}$ entry in the product matrix, we calculate this by multiplying each element in the $i^{th}$ row of $\mathbf{A}$ by each element in the $j^{th}$ column of $\mathbf{B}$ and taking the sum
- For example, $\mathbf{C_{2,1}}$ is calculated by multiplying the elements in the $2^{nd}$ row of $\mathbf{A}$ by each element in the $1^{st}$ column of $\mathbf{B}$ and taking the sum
- For example, $\mathbf{C_{1,3}}$ is calculated by multiplying the elements in the $1^{st}$ row of $\mathbf{A}$ by each element in the $3^{rd}$ column of $\mathbf{B}$ and taking the sum
- Notice that you are essentially taking the dot product of the $i^{th}$ row of $\mathbf{A}$ and the $j^{th}$ column of $\mathbf{B}$ (see the previous sub-section on dot products)


<b>Note:</b> highly recommend watching the video attached at the end of section 2 covering the preface to gain a better understanding of matrix multiplcation! 
<h4 id="2e_cell"> <span style="color: #f28f2c;"> 2e. Rules for matrix addition and multiplication </span> </h4>

Assume we have the following matrices with specific dimensions: 
- $\mathbf{A}$ matrix of $m \times n$
- $\mathbf{B}$ and $\mathbf{D}$ are matrices of $n \times r$
- $\mathbf{C}$ matrix of $r \times s$
- $\mathbf{I}$ is an identity matrix
- $k$ and $j$ are real numbers 

There are a few properties/laws governing matrix operations that may simplify your calculation processes: 
1. $\mathbf{A(BC)} = \mathbf{(AB)C} \rightarrow $ Associative property of multiplication
2. $\mathbf{A(B+D)} = \mathbf{AB} + \mathbf{AD} \rightarrow $ Distributive property of multiplication
3. $(k+j)\mathbf{A} = k\mathbf{A} + j\mathbf{A} $
4. $(k\mathbf{A})\mathbf{B} = k\mathbf{(AB)} = \mathbf{A}(k\mathbf{B}) $
5. $\mathbf{AI} = \mathbf{IA} = \mathbf{A} \rightarrow $ Identity matrix behaves like the real number 1

<h4 id="2f_cell"> <span style="color: #f28f2c;"> 2f. Graph minimums/maximums and gradients  </span> </h4>

<b>What is a minimum and maximum?</b>

The minimum is the point where a function $f(x)$ reaches a lowest value either locally or globally. On the otherhand, the maximum is the opposite and refers to the point where a function $f(x)$ reaches a highest value, locally or globally. Visually, they appear like the bottom of a valley or the top of a hill respectively as shown in the figure below: 

<div align=center>

![image-2.png](attachment:image-2.png)

<b><i>Figure 1. Graph extrema. </i></b>

</div>


The minima and maxima of a graph can be referred to collectively as <b>extrema</b>. We previously stated that there can be either: 
- <b>Local extrema:</b> the extreme point of a function within a limited range 
- <b>Global extremum:</b> the extreme point of a function over its entire domain; the absolute highest or lower point of the function considering all possible inputs 


<b>How can we find the graph extremum?</b>  

From the figure above, we can see that our graph extrema are not always in the shape of 'valleys' or 'hills' but they can also be located at the boundary of a function. In general, for a continuous function with a single closed interval $f(x): [a,b] \in R$, there are three possible graph extrema: 
- Stationary points
- Boundary points 
- Singular points

<b>Stationary points: </b>  
You may have heard of maximums and minimums being referred to as <b>turning points</b> and these are essentially what stationary points are. They are the points of a function where the gradient/slope is equal to zero i.e., the point ($x, f(x)$) when the gradient $\frac{df(x)}{dx} = 0$. 

The terms stationary point and extrema may seem interchangeable, and this was currently the case as we were dealing with only maximum and minimum points. However, notice in the figure below that asides from minimum and maximum turning points, we can also have reflection points where the gradient is zero at the reflection point but this is not the minimum or maximum value of the function within its defined boundaries. What we are trying to get across is that stationary points are not necessarily extrema (and vice versa) due to the exception of reflection points. 

<div align=center>

![image.png](attachment:image.png)

<b><i>Figure 2. Stationary points: minimums, maximums, and reflection points </i></b>

</div>

<b>Boundary points</b>  

The graph in the top right of figure 1 demonstrates how continuous functions within closed boundaries can have their global/local minimum or maximums at the closed boundary i.e., the global minimum occurs at $x=a$. 

<b>Singular points</b>

So far, we have mentioned that we can either locate graph extrema at the boundary points of a closed continuous function or where the gradient is equal to zero (with the exception of reflection points), however, we can also have graph extrema at points where the gradient does not actually exist.  

<div align=center>

![image-2.png](attachment:image-2.png)

<b><i>Figure 3. Modulus function and singular points. </i></b>

</div>

Figure 3 above is an example of a 'modulus' function where the global/local minimum appears to be at the corner of the function where $x=0$. In this case, the modulus function is what we describe as 'not differentiable' because the derivative is not fully continuous. 
- On the LHS of the function where $x<0$, the gradient is equal to $-1$ (decreasing constantly)
- On the RHS of the function where $x>0$, the gradient is equal to $1$ (increasing constantly)
- At the point where $x=0$, we cannot determine what the gradient of the function is as it suddenly jumps from $f'(x)=-1$ to $f'(x)=1$. In other words, it is 'discontinuous'. 

However, from our visual interpretation of the graph, we can still see that this is a local/global extremum. 

After obtaining all possible points above, we can compute the values of all these points under the function $f(x)$. The highest value is the maximum, whereas the lowest value is the minimum. 
<h4 id="2g_cell"> <span style="color: #f28f2c;"> 2g. Derivatives </span> </h4>

Given a function $f(x)$, the derivative of the function with respect to $x$ (denoted as $f'(x)$ or $\frac{df}{dx}$) represents how the function's value changes as $x$ changes (the rate of change). The process of finding a derivative is called <b>differentiation</b>, and the reverse process is called <b>integration</b>. We have mentioned how the gradient can be used to determine the stationary points of a function, and in the context of training neural networks, these derivatives are actually calculated and used to minimise the loss of a model (effectively enabling it to learn), but how do we actually determine derivative functions? 

The most common types of functions have a generalised formula for their derivatives as shown in the figure below: 

![image.png](attachment:image.png)

For more complex functions, we can use laws and properties of differentiation to calculate their derivatives: 

![image-2.png](attachment:image-2.png)


<h4 id="2h_cell"> <span style="color: #f28f2c;"> 2h. Sigma notation </span> </h4>

<b>What does SIGMA notation mean?</b>

Sigma notation (uppercase symbol for sigma), also known as summation notation, represents the <b>sum of the terms listed in the function</b>: 

$$
\sum_{x=a}^{z} f(x) = f(a) + f(b) + ... + f(y) + f(z) 
$$

- $f(x)$ represents the current term of the series 
- $x$ is the index variable, which takes on integer values starting from $a$ and increasing by 1 until it reaches $z$
- $a$ is the lower bound of the summation
- $z$ is the upper bound of the summation

<h4 id="2i_cell"> <span style="color: #f28f2c;"> 2i. Pi notation </span> </h4>

<b>What does PI notation mean?</b>

Pi notation (uppercase symbol for pi) represents the <b>product of the terms listed in the function</b>: 

$$
\prod_{x=a}^{z} f(x) = f(a) \cdot f(b) \cdot ... \cdot f(y) \cdot f(z) 
$$

Similar to the SIGMA notation: 
- $f(x)$ represents the current term of the series 
- $x$ is the index variable, which takes on integer values starting from $a$ and increasing by 1 until it reaches $z$
- $a$ is the lower bound of the multiplication
- $z$ is the upper bound of the multiplication
<h4 id="2j_cell"> <span style="color: #f28f2c;"> 2j. Videos </span> </h4>

Preface section (video_length): {insert_video_link}
<h2> <span style="color: #f28f2c;"> ~~~ Part 3. Machine Learning ~~~ </span> </h2>
<h4 id="3a_cell"> <span style="color: #f28f2c;"> 3a. What is AI and Machine Learning? </span> </h4>

Artificial Intelligence and Machine Learning are two terms that are often used interchangeably. While these terms are related in concepts, both have different meanings. 

<b>Artificial intelligence (AI)</b> refers to the broader concept of machines or computer systems performing tasks that typically require human intelligence and have the ability to mimic cognitive functions such as being able to see, understand and respond to spoken/written language, analyse data or make recommendations.

<b>Machine learning</b> is a subset of AI which enables a machine or a system to learn and improve from experience. It focuses on the development of algorithms and statistical models that enable computers to perform tasks without being explicitly programmed. The main takeaway from machine learning is that machines are able to learn from data and improve their performance over time. 

When developing these machine learning systems, we need to feed the system with a <b>dataset</b> which is a collection of data used for training, testing, and evaluating a machine learning model. Typically, a dataset consists of two main components: features and labels.


<table>
    <tr>
        <th> Components </th>
        <th> Description </th> 
    </tr>
    <tr>
        <td> Features </td>
        <td> Variables that describe the input data which can be in the form of numerical, categorical, or even text-based variables.</td>
    </tr>
    <tr>
        <td> Labels </td>
        <td> Values that the model aims to predict or classify </td>
    </tr>
</table>

Let's consider the example of a housing price prediction dataset. If we were tasked with selecting a random house in Victoria and guessing how much the house costs, how would you make this decision? You would look at the characteristics of the house that define how luxurious it is such as: 
- the size
- the number of bedrooms 
- the location, etc.

In the machine learning context, these characteristics represent our features and the labels would be the actual prices of the houses corresponding to the value of the features. A house with one bedroom and one bathroom (the features) would have a lower cost (the label) compared to a house with a pool and a gym. 

When training a neural network, we often divide the dataset into two subsets: the training set and the test set. 
- The training set is used to train the machine learning model by learning patterns and relationships. 
- The test set is used to evaluate the model’s performance against new and unseen data, indicating how well it generalizes to patterns it hasn't encountered during training. Typically, the training set makes up 80% of the dataset and the test set makes up the other 20%.

<b>Just a quick teaser into machine learning whilst the idea of a housing price prediction model is fresh in your mind:</b> let's say you are given an advertisement for a house by your supervisor where its included features are listed and your job is to predict the cost of the house. From your own tuition, you will make a reasonable deduction but it doesn't end there! Remember that the purpose of machine learning models is to learn! How do we know whether our prediction was correct or wrong? How far away from the 'correct' answer were we?  

Your supervisor will give you the actual house pricing from the advertisement and you will start to build a reference for housing prices based on their listed features. This process is repeated and is essentially how a machine learning model learns:  
1. look at more housing advertisements (the features from the dataset)
2. make more predictions (experimental labels)
3. find out whether you were right or wrong (compare experimental and actual labels from the dataset), how far off you were
4. re-adjust your next predictions based on your previous mistakes 
5. look at more housing advertisements, and repeat until you become better and better at predicting housing prices   
<h4 id="3b_cell"> <span style="color: #f28f2c;"> 3b. Types of learning </span> </h4>

So far, we have discussed how a machine is able to learn from the dataset in the previous section. This approach is described as 'supervised learning' and from the title, we can guess that there are a couple other ways for machine learning models to learn. The other two types of learning we will be discussing are 'unsupervised training' and 'reinforcement learning'. Throughout this section, let’s imagine how a machine learning model can learn about a basket of fruits which includes apples, bananas, and oranges depending on the information given. 

<b>Supervised training: </b>  

Imagine each fruit is labelled with its name and you want to teach a computer to recognise these fruits. You show the computer the labelled dataset, indicating which fruit is which. The algorithm learns to associate the features of each fruit (colour, shape, and size) with the corresponding label (apple, banana, or orange). After training, the computer should be able to accurately identify new, unseen fruits based on the patterns it learned during training. This is called supervised training. 

Therefore, supervised learning involves training the algorithm on a labelled dataset, where the input data is paired with corresponding output labels. The goal is for the algorithm to learn a mapping function that can accurately predict the output (y) for new, unseen data (x).

<b>Unsupervised training: </b>

Now let’s say you have the same basket of fruits, but this time, none of them are labelled. You want the computer to discover if there are any natural groupings or similarities among the fruits. So, the computer explores the dataset without any labels. It might identify that some fruits share similar features and group them together such as bananas having a curved elongated shape unlike apples and oranges. The goal is to uncover hidden patterns or structures within the data, clustering similar fruits together without predefined categories. This is called unsupervised training. 

Therefore, unsupervised learning involves training the algorithm on unlabelled data, and the system tries to find patterns, relationships, or structures within the data without explicit guidance. The goal is often to discover inherent structures or groupings in the data.

<b>Reinforcement learning: </b>

Now consider a scenario where you have a robotic arm that can pick fruits from the basket. The goal is to teach the robot to pick the ripest fruits while avoiding unripe ones. The robot performs actions (picking a fruit) in an environment (the basket) to maximise a cumulative reward (picking up ripe fruits and avoiding unripe ones). The robot receives feedback in the form of a reward or penalty based on the success or failure of its actions. The robot learns over time to select actions that lead to higher rewards, ultimately becoming proficient at picking ripe fruits. This is called reinforcement learning. The main difference between reinforcement and supervised/unsupervised learning is that the model does not require a labelled dataset or training set. 

Therefore, it is a type of learning where an agent interacts with an environment and learns to make decisions by receiving feedback in the form of rewards or penalties. The agent aims to maximize cumulative reward over time. 

The table below is a nice summary comparing the three different types of machine learning: 

![figure_1.png](attachment:figure_1.png)

Reference: https://www.linkedin.com/pulse/supervised-vs-unsupervised-reinforcement-learning-gurvinder-sethi/ 
<h4 id="3c_cell"> <span style="color: #f28f2c;"> 3c. Types of models </span> </h4>

Within machine learning, there are fundamental components that may need to predict numerical values or classify objects into different categories. We’ll speak about two types of models that can be applied in machine learning named after the task they are trying to perform at hand: linear regression models and classification models. 

<b>Linear regression models</b> attempt to draw straight lines to represent the relationship between the input and output variables ($y=mx+c$) given the assumption that there is a linear relationship between the input features and continuous numerical output. For example, let's simplify our house pricing dataset to include only one feature (housing sizes in square metres) and their corresponding prices. In linear regression, the goal is to create a linear relationship between the size of the house and its price where we can assume that the bigger the house, the higher the price (hopefully at a constant rate defined by the gradient of the relationship). 

<b>Classification models</b> attempt to assign input data to predefined categories or classes. In other words, the model is trying to predict if an output fits into a certain category based on predefined features. Consider a dataset of emails labelled as either "spam" or "not spam." In a classification model, the goal is to build a model that can predict whether a new email is spam or not based on features like the presence of certain keywords, sender information, etc. There are various classification models including logistic regression, decision trees, neural networks etc. These models learn patterns from the training data and create decision boundaries to separate different classes. Some of these models will be discussed in later notebooks. 
<h4 id="3d_cell"> <span style="color: #f28f2c;"> 3d. Linear regression </span> </h4>

<h4> <span style="color: #f28f2c;"> 3d (i) Loss functions and optimization</span> </h4>

To reiterate the objective of linear regression models, they aim to represent linear relationships between the input and output variables assuming that there is a linear relationship between the two variables. Essentially, we want to represent the data as a straight line in the form:
 
$$y=wx+\beta$$ 

where $y$ is the output, $x$ is the input, $w$ is the weights (gradient), and $\beta$ is the bias (y-intercept/initial value). 

<b>Note:</b> you may be more familiar with the terms used in the brackets (gradients, intercepts, and $y=mx+c$); however, we are introducing the terms, weights and biases, as these are used when describing the parameters of a machine learning model. Our ultimate goal is to be able to determine the values for $w$ and $\beta$ that will accurately allow the model to generate a prediction $y$ from any given input $x$. 

<div align='center'>

![image.png](attachment:image.png)

<b>Figure 4. General linear equation ($y=x+1$)</b>

</div>

Now, consider some data with values for both a predictor variable (x input) and response variable (y output) as shown in figure 5 below (accompanied by its code). The red line represents a linear equation ($y=2x+1$) which was similarly seen in figure 4 above. The blue dots were generated by taking each of the points on the red line and adding a random amount of noise to replicate real-world data that will be naturally noisy. 
# Importing required libraries 
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(0)

# Generating our true (ideally linear) and noisy (realistic) data
x = np.linspace(1, 10, 10)
y_true = 2 * x + 1
y_sim = y_true + np.random.normal(0, 2, y_true.shape)

# Plotting 
plt.plot(x, y_true, label='true data', color='red')
plt.scatter(x, y_sim, label='simulated data', color='blue')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('Figure 5. Effect of input (x) on the output (y)')
plt.show()
<b>Loss functions</b> 

If we think the relationship between the input and output follows a straight line, we can fit a line using a parametric approach. From our line equation, we know that we have two parameters to estimate to construct the function: $w$ and $\beta$, the weights and biases respectively. The main approach to determine optimised values for these values from our raw training data above is through a loss function, which essentially tells us how far off a predicted value deviates from the actual value. For example, a simple loss function might be one that determines the difference (residual) between the actual sample value and predicted value. 

In simpler terms: "how far off was our prediction from the actual value?"

$$Residual = y_{true} - y_{predicted} = y_{true} - (wx+\beta)$$

By summing all the residuals together between different samples, we can evaluate how much the predicted straight line differs from the actual data. If we are able to minimise this loss function, we can find a more accurate estimate for our weights and biases. This process is referred to as <b>optimization</b>. 
<b>Least Squared Error (LSE) function</b>

Instead of the sum of residuals, we typically use the sum of squared residuals as it has more favourable mathematical properties. For example, squaring the residuals 
removes the impact of positive and negative residuals by looking at only the magnitudes. This is known as the least squared approach where we are trying to minimise the sum of the squared residuals: 
1. Find the differences (residuals) between the predicted and true value for each sample 
2. Square each of the differences
3. Add together the squared differences

$$LSE = (y_{1} - \hat{y_{1}})^2 + (y_{2} - \hat{y_{2}})^2 + ... + (y_{n} - \hat{y_{n}})^2 = \sum_{i=1}^{n} (y_{i} - \hat{y_{i}})^2 $$

where $y_{i}$ is the actual value and $\hat{y_{i}}$ is the predicted value for a given sample $i$.
But how do we relate this loss function back to our weights and biases parameters? We need to minimise the LSE by taking the derivative of the LSE with respect to our parameters $\left(\frac{dLSE}{dw} \text{ and } \frac{dLSE}{d\beta}\right)$ and solving for the minimum. To simplify this process, we can use the following formulas to derive our weights and biases respectively: 

$$w = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}$$

$$\beta = \bar{y} - w\bar{x}$$

where $\bar{x}$ and $\bar{y}$ are the mean input and output values respectively. 
For our simulated data, we can estimate our parameters and create a line using the code below. Note that we know our actual simulated function is $y=2x+1$, but our data (blue dots) has some noise that simulates the effect of sampling (random nose). Let's see how close our prediction (red line) is to the actual simulated function (green line): 
# Importing required libraries 
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
np.random.seed(0)


# Creating our true and noisy data
x = np.linspace(1, 10, 10).reshape(-1,1)
y_true = 2 * x + 1
y_sim = y_true + np.random.normal(0, 2, y_true.shape)

# Creating our approximation 
model = LinearRegression().fit(x, y_sim)

# Simulated data points
plt.title('Figure 6. Comparing least squares line to the theoretically linear relationship')
plt.xlabel('x')
plt.ylabel('y')
plt.scatter(x, y_sim, label='Simulated data', color='blue')
# Least squares line 
plt.plot(x, model.predict(x), label='Least squares line', color='red')
# Actual relationship
plt.plot(x, y_true, label='Theoretical relationship', color='green')
plt.legend()
plt.show()
<h4> <span style="color: #f28f2c;"> 3d (ii) Increasing dimensionality; Multiple Linear Regression </span> </h4>

For our simple model and loss function, we were able to find a clear formula for a minimum. This will not be the case for more complicated models that may require more iterative approaches to find a minimum. That being said, linear regression can help us determine a relationship between one feature and one label. But what if we want to use multiple features to predict a single label?  

Turns out, it is easy to perform multiple linear regression to address cases where we have multiple features contributing to one label. In the real world, most responses will have multiple causes. The form of the function is just an extension to the one we find in simple linear regression. The below formula refers to a multiple linear regression model with $n$ features where $\beta_{n}$ refers to the $n^{th}$ weight value(co-efficient) and $x_{n}$ refers to the $n^{th}$ feature. 

$$ y = \beta + w_1x_1 + w_2x_2 + ... + w_nx_n$$

The same least squares aproach and RSS loss function is used; however, weight and bias estimates are not as simple and require matrix algebra. For brevity, the exact mathematical derivation is not shown. In addition, there are more considerations to make for validation of multiple regression models such as whether the features are actually correlated with each other, but this is beyond the scope of this notebook. Below is a multiple linear regression simulated dataset and model with two features ($x_1$, $x_2$) and one label ($y$). 
# Importing required libraries 
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
#np.random.seed(0)

# Generating our noisy data sample (3D data)
x1, x2 = np.random.rand(2, 100) * 10
y = 1.5 + 2 * x1 + 3 * x2 + np.random.normal(0, 2, 100)

# Plotting our noisy data sample
fig, ax = plt.subplots(subplot_kw={'projection': '3d'})
ax.scatter(x1, x2, y, c='r', marker='o')
ax.set(xlabel='$x_1$', ylabel='$x_2$', zlabel='y', title='Figure 7. 3D Plot for Simulated Data')

# Generating our prediction plane  
X = np.stack((x1, x2), axis=1)
model = LinearRegression().fit(X, y)

# Plotting our predicted plane 
x1_mesh, x2_mesh = np.meshgrid(np.linspace(x1.min(), x1.max()), np.linspace(x2.min(), x2.max()))
ax.plot_surface(x1_mesh, x2_mesh, model.predict(np.stack((x1_mesh.ravel(), x2_mesh.ravel()), axis=1)).reshape(x1_mesh.shape), alpha=0.5, color='b')
plt.show()
<h4 id="3e_cell"> <span style="color: #f28f2c;"> 3e. Underfitting and overfitting </span> </h4>

<div align=center>

![figure_2-2.jpg](attachment:figure_2-2.jpg)

<b><i>Figure 8. Evaluating the machine learning model output against the original data. </i></b>

</div>

When our machine learning model tries to make patterns out of data it is given access to, the model can either underfit, overfit or optimally find the right pattern fit to the data.

<b>Overfitting:</b>  
This is evident when the relationship made by the model is trying to capture all the data, regardless of the outliers. This typically happens if the data provided to the model is too specific or too few datapoints are being used by the model and it is mostly <b>memorising</b> what data to expect instead of learning patterns and complexities in the data. There are a number of techniques to address overfitting models. For example, we can increase the number and variations (data augmentation) of the data samples to expose the model to more generalised patterns that it can learn from. Remember that ultimately, our goal is to produce a model that is able to accurately predict <b>unseen data</b>. If this model was only memorising the training dataset instead of learning complicated patterns, it will struggle to perform accurately when presented with unseen data. 

<b>Underfitting:</b>  
When models are not learning the right patterns in data or are not learning enough from it, they predict underfitted patterns. This can happen for a number of reasons but mainly if the model is not sophisticated enough to detect the right patterns from data (fitting a linear model to non-linear data), feature selection, model parameters, insufficient training, and poor data quality. Hence, a couple of techniques to address models that are underfitting include producing more complex models that capture more patterns (e.g., introducing more hidden layers), data processing (noise filtering) before training, tuning hyperparameters, using different models, etc.
<div style="background-color: #de8d2a; color: white; padding: 10px; border-radius: 5px;">

<b>'The Ideal Model:'</b>

Ideally, we want a model that is just complex enough to recognise complicated patterns and features in the dataset whilst still being simple and generalised enough to perform as well on unseen data containing outliers that were not in the original dataset. Later when we start going through neural network models, we will learn about how we split our dataset into training and validation sub-datasets that are used for training and evaluating on unseen data respectively. By analysing the performance of the model on these two datasets, we will be able to determine whether our model is overfitting or underfitting: 
- <b>Underfitting:</b> poor performance on both training and validation sets. 
- <b>Overfitting:</b> excellent performance on the training set but poor performance on the validation set. 
<h4 id="3f_cell"> <span style="color: #f28f2c;"> 3f. Logistic regression </span> </h4>
<h4> <span style="color: #f28f2c;"> 3f (i) Logistic function and Log-Loss </span> </h4>

Logistic regression is a binary classification model: it accepts a number of features and produces a binary output from these features such as yes or no, up or down, 0 or 1. For example, what is the probability of me having cancer if I have the following symptoms (features)? Unlike a linear regression where the dependent variable is a continuous value, logistic regression is used to make a prediction about a categorical variable. Similar to how we had a linear regression model ($y=wx+\beta$), we use a logistic function to model the probabilty of an event occurring given contributions from the features:  

$$f(x) = \frac{1}{1 + e^{-(\beta + w_1x_1 + w_2x_2 + ... + w_nx_n)}}$$

Figure 9 below graphically summarises the difference between a linear regression (blue line) and logistic regression (orange line) model: 

<div align="center">

![figure_3.png](attachment:figure_3.png)

<b>Figure 9. Comparison between linear regression and logistic regression model </b>

</div>

We can see that the logistic function can be a useful model for our data. For example: 
- our output values are restricted between 0 and 1 (probability values)
- the function's sigmoid shape is able to model extreme values between 0 and 1 
- there is a cutoff point (p=0.5) that will enforce our binary model to only produce discrete outputs (0 or 1) 

<b>Note:</b> similar to our linear regression model, we have made the assumption that the data can accurately be represented with a logistic regression model. Mainly, the model is able to take upon extreme values; however, it is unable to model turning points that we might find in more complex models. 

Now that we have a model function for our logistic regression model, we need a loss function to optimize the weights and biases. For our linear regression model, this was originally the Least Squared Error function; however, this is inappropriate for logistic regression. Instead, we use the <b>Logistic Loss</b> as we want to make the predictions: 
- as close to 1 when the outcome is 1, and 
- as close to 0 when the outcome is 0

The Logistic Loss function (also called the cross-entropy loss) is given by: 

$$\text{Log-Loss} = \sum_{i=0}^{n} -[y_i log(\hat{y_i}) + (1-y_i) log(1-\hat{y_i})]$$

$$\text{Log-Likelihood} = \sum_{i=0}^{n} [y_i log(\hat{y_i}) + (1-y_i) log(1-\hat{y_i})]$$


where $y_i$ and $\hat{y_i}$ are the true and predicted label values for sample $i$. 
 
<b>Note:</b> minimizing the Log-Loss is equivalent to maximising the Log-Likelihood since the Log-Loss is the negative of the Log-Likelihood (notice the negative multiplication)
<h4> <span style="color: #f28f2c;"> 3f (ii) Optimizing the Log-Loss function </span> </h4>
<b>Now that we have the Log-Loss function, how do we go about minimising this? </b>

This is achieved through an optimization method called <b>gradient descent</b> which is essentially described by its name. First, we calculate the partial derivative (gradients) of the Log-Loss function with respect to each weight (coefficient) and bias value in a process called <b>backpropagation</b> (to be covered in more detail in Notebook 2). Then, starting from a chosen initial value, we will use the partial derivatives (gradients) to determine which direction the loss function decreases (descending) and move in that direction by a specific step size (specified by the <b>learning rate</b>). We repeat this process for a specified number of iterations, updating our parameters and hopefully getting closer and closer to the minimum loss value (converging). 

The partial derivatives for the loss function with respect to the weight and bias are denoted by $\frac{\partial L(w)}{\partial w}$ and $\frac{\partial L(b)}{\partial b}$ respectively. Gradient descent progressively updates the weight and bias values according to a specified learning rate $\eta$ (eta) to minimise the Log-Loss until the values converge to a local/global minimum: 

$$w := w - \eta \frac{\partial L}{\partial w}$$
$$b := b - \eta \frac{\partial L}{\partial b}$$

The GIF below is a visual representation of what occurs ideally during each iteration of gradient descent: 

<div align=center>

![GIF_1.gif](attachment:GIF_1.gif)

<b><i>Figure 10. Gradient descent to a global minimum. </i></b>

</div>


<b>Stochastic gradient descent: </b>

Another iterative and closely related method to gradient descent is stochastic gradient descent, which has a different set of update rules. Instead of taking into account the entire dataset, it uses a pre-selected number of data points to compute its new weight and bias values for each iteration until convergence. This is clearly more efficient, especially for models with a large number of parameters. 

$$ w := w - \eta (p^{(i)} - y^{(i)}) x^{(i)} $$
$$ b := b - \eta (p^{(i)} - y^{(i)}) $$
The following [link here](https://mlu-explain.github.io/logistic-regression/) is a good interactive website that nicely runs through logistic regression. Definitely recommend having a look :) 
<h2> <span style="color: #f28f2c;"> ~~~ Part 4. Recap and future steps ~~~ </span> </h2>
Congratulations on making it to the end of our very first Monash AIM notebook. We have a discussed a large number of topics to help with on-boarding new members of the team, accounting for the diverse range of experiences they may be coming from: 
-	An introduction to vectors and matrices, matrix operations, graph extrema, gradients, derivatives, sigma notation, and pi notation. 
-	What is the difference between AI and Machine Learning? 
-	What is a dataset? (along with a housing price prediction model example) 
-	The difference between supervised, unsupervised, and reinforcement learning
-	When we need a linear regression model as opposed to a classification (logistic regression) model
    - What model function and loss function is associated with each? (Linear equation + least squared error in comparison to logistic function + log-loss)
    - What is the purpose of these loss functions and how can they be used for optimization? 
-	Underfitting and overfitting models 
-   Brief introduction to backpropagation and gradient descent 

<div style="background-color: #de8d2a; color: white; padding: 10px; border-radius: 5px;">

<b>So what is the next step?</b>

You have seen what we can do with simple neural network models such as fitting a straight line to noisy data (linear regression) and the important role that loss functions play in optimizing these models for better performance. These are very powerful tools and in today's world, we have a large number of diverse models that have been developed to perform much more complex and amazing operations. Clearly, we are not expecting each and every one of you to be able to remember how all of these different models work, similar to how we don't expect anyone to remember every single word in the dictionary. However conveniently enough, these different neural networks are built upon from the same foundational structures. The reason they are 'deep neural networks' is because of how far people can extend them beyond their foundations to achieve greater things. 

These foundations are what we will be introducing you to in our next notebook on Neural Networks. You have already encountered some of them briefly in this notebook (loss functions, optimization, backpropagation, and gradient descent) but we will discuss these in a broader scope and piece them together in a neater fashion in the next notebook. 
<div align=center>

{Throw in GITHUB permalink to Notebook 2 when finalised}

</div>
